{"cells":[{"cell_type":"markdown","metadata":{"id":"a59UKY4T2lk_"},"source":["# ABOUT"]},{"cell_type":"markdown","metadata":{"id":"DX2lsmPYcVwp"},"source":["\n","Datascientest's Datascientist continuous bootcamp - cohorte Mars2022 -  AeroBOT project\n","\n","**Tutor**\n","\n","* Alban THUET\n","\n","**Authors:**\n","\n","* Hichem HADJI  \n","\n","</br>\n","\n","---\n","</br>\n","\n","**Version History**\n","\n","Version | Date       | Author(s)  | Modification\n","--------|----------- | ---------  | --------------------------\n","1.0     | 20/10/2022 | H.H       | modif"]},{"cell_type":"markdown","metadata":{"id":"vKhxiy5acdjC"},"source":["This notebook can be executed entirely. \n","\n","It\n","\n","* mounts the GDrive of our AeroBot project @gmail account.\n","\n","* loads the data from the `df_for_Anomaly_prediction.pkl` under a pandas DataFrame named `df`, which contains `96986` entries and 20 columns.\n","These data do not contain any more UAS-related entries.\n","* A comparison of 2 naive_bayes approaches while using 2 different vectorizers (Count_Vectorizer vs tfidf_vectorizer) while working with different max_features (500, 1000, 3000) "]},{"cell_type":"markdown","metadata":{"id":"tAJ40jap2rBh"},"source":["# IMPORT PACKAGES\n"]},{"cell_type":"markdown","metadata":{"id":"qH7RERJu3N0Z"},"source":["settings for  full / patial Narrative display. Helene?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2825,"status":"ok","timestamp":1666276182059,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"},"user_tz":-60},"id":"5_efzqI3_FPo","outputId":"ab5d9ad8-0e2c-4f42-87f3-c35904570353"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["#######################\n","# Import packages\n","#######################\n","import numpy as np\n","import seaborn as sns\n","\n","#######################\n","# Pandas\n","#######################\n","import pandas as pd\n","# Set pandas settings to show all data when using .head(), .columns etc.\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None\n","pd.set_option(\"display.colheader_justify\",\"left\") # left-justify the print output of pandas\n","\n","### Display full columnwidth\n","# Set pandas settings to display full text columns\n","#pd.options.display.max_colwidth = None\n","# Restore pandas settings to display standard colwidth\n","pd.reset_option('display.max_colwidth')\n","\n","import itertools # Pour créer des iterateurs\n","\n","# Package to show the progression of pandas operations\n","from tqdm import tqdm\n","# from tqdm.auto import tqdm  # for notebooks\n","\n","# Create new `pandas` methods which use `tqdm` progress\n","# (can use tqdm_gui, optional kwargs, etc.)\n","tqdm.pandas()\n","# simply use .progress_apply() instead of .apply() on your pd.DataFram\n","\n","######################\n","# PLOTTING\n","######################\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# Define global plot parameters for better readability and consistency among plots\n","# A complete list of the rcParams keys can be retrieved via plt.rcParams.keys() function\n","plt.rcParams['axes.titlesize'] = 30\n","plt.rcParams['axes.labelsize'] = 23\n","plt.rcParams['xtick.labelsize'] = 23\n","plt.rcParams['ytick.labelsize'] = 23\n","plt.rc('legend', fontsize=23)    # legend fontsize\n","\n","# BOKEH \n","from bokeh.plotting import figure # Importation de la classe figure qui permet de créer un graphique bokeh.\n","from bokeh.io import  push_notebook, output_notebook, show\n","output_notebook() # permet d'afficher tous les futurs graphiques dans l'output d'une cellule jupyter. Si cette instruction n'est pas lancée, la figure s'affichera dans un nouvel onglet.\n","from bokeh.models import ColumnDataSource\n","from bokeh.transform import dodge\n","from bokeh.models.tools import HoverTool\n","\n","#####################\n","# NLP \n","#####################\n","import re # for Regular Expression handling\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet') # WordNet lemmatizer\n","nltk.download('omw-1.4') # necessary for WordNet lemmatizer\n","from nltk.tokenize import word_tokenize # Usual tokenizer\n","from nltk.tokenize import TweetTokenizer # Special tokenizer;  \"we'll\", \"didn't\", etc. are considered as one word\n","from sklearn.feature_extraction.text import CountVectorizer # Vectorization\n","from nltk.corpus import stopwords # Import stopwords from nltk.corpus\n","from nltk.stem.snowball import EnglishStemmer\n","\n","######################\n","# Naive bayes\n","######################\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","##############################\n","# Undersampeling\n","##############################\n","from imblearn.under_sampling import RandomUnderSampler \n","\n","###############################\n","# ML preprocessing and models\n","###############################\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import ensemble # random forest\n","from sklearn.svm import SVC\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","import pickle as pkl"]},{"cell_type":"markdown","metadata":{"id":"cjRrSdqQ7llR"},"source":["# LOAD DATA"]},{"cell_type":"markdown","metadata":{"id":"BfmhmLv0_Osg"},"source":["## Mount GDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":19573,"status":"ok","timestamp":1666276205788,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"},"user_tz":-60},"id":"N_mjKklM_bJH","outputId":"aef11cd4-8a84-466d-c468-2d279f288623"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["#@title\n","# Mount your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#check your present working directory \n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666276205789,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"},"user_tz":-60},"id":"S72xAGPS_bGM","outputId":"8a3cf493-1f2c-4f0a-e16a-2ef84db75310"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed\n"]}],"source":["#@title\n","# move to the transformed data location (you can create a deeper structure, if needed, e.g. to save a trained model):\n","%cd /content/drive/MyDrive/data/transformed/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666276205789,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"},"user_tz":-60},"id":"NkiTSU2c_bDh","outputId":"a614f555-cab5-44c0-c66b-06b8ef516b0e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 2022_09_11_7_4_3_raw_narr_BERT_BASE_frozen_max_length_345.pkl\n"," complaints-2022-08-05_13_55.csv\n","'Copy of Qualified abbreviations_20220718.xlsx.gsheet'\n","'Data Dictionnary.xlsx'\n"," data_for_BERT_multilabel_20220805.pkl\n"," df_for_Anomaly_prediction.pkl\n"," model.png\n"," model_results\n"," Narrative_PP_stemmed_24072022_TRAIN.pkl\n"," Narrative_Raw_Stemmed_24072022_TRAIN.pkl\n"," Narrative_RegEx_subst_21072022_TRAIN.pkl\n","'Qualified abbreviations_20220707_test.csv'\n","'Qualified abbreviations_20220708.csv'\n","'Qualified abbreviations_20220718.csv'\n","'Qualified abbreviations_20220718_Google_sheet.gsheet'\n"," test_data_final.pkl\n"," train_data_final.pkl\n"]}],"source":["#@title\n","!ls # list the content of the pwd\n","\n","#!ls \"/content/drive/MyDrive/Data_Science/Formations/DataScienceTest/projet/AeroBot/\" # list contect of a speficic folder"]},{"cell_type":"markdown","metadata":{"id":"pB8SGMTD4JsN"},"source":["## Load data from .pkl file\n"]},{"cell_type":"code","source":["# Load the TRAIN data (97417 entries)\n","# Do not touch the TEST data until the end of the project!\n","# or the curse of the greek gods will fall upon you!\n","\n","%cd /content/drive/MyDrive/data/transformed/\n","with open(\"df_for_Anomaly_prediction.pkl\", \"rb\") as f:\n","    loaded_data = pkl.load(f)\n","\n","df = loaded_data\n","print(\"\\nA Dataframe with\", len(df), \"entries has been loaded\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzIJjDFRCwv4","outputId":"f7337719-1982-4721-e22c-93a08aa9d75f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed\n"]}]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPD1x3NZC01-","executionInfo":{"status":"ok","timestamp":1666196553579,"user_tz":-60,"elapsed":207,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"f6c7710e-f2bd-4951-8237-e45a393defc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 96986 entries, 1163382 to 874642\n","Data columns (total 20 columns):\n"," #   Column                                        Non-Null Count  Dtype \n","---  ------                                        --------------  ----- \n"," 0   Narrative                                     96986 non-null  object\n"," 1   Anomaly                                       96986 non-null  object\n"," 2   Narrative_PP_stemmed                          96986 non-null  object\n"," 3   Narrative_Raw_Stemmed                         96986 non-null  object\n"," 4   Narrative_Raw_Stemmed_str                     96986 non-null  object\n"," 5   Narrative_PP_stemmed_str                      96986 non-null  object\n"," 6   Anomaly_Deviation / Discrepancy - Procedural  96986 non-null  int64 \n"," 7   Anomaly_Aircraft Equipment                    96986 non-null  int64 \n"," 8   Anomaly_Conflict                              96986 non-null  int64 \n"," 9   Anomaly_Inflight Event / Encounter            96986 non-null  int64 \n"," 10  Anomaly_ATC Issue                             96986 non-null  int64 \n"," 11  Anomaly_Deviation - Altitude                  96986 non-null  int64 \n"," 12  Anomaly_Deviation - Track / Heading           96986 non-null  int64 \n"," 13  Anomaly_Ground Event / Encounter              96986 non-null  int64 \n"," 14  Anomaly_Flight Deck / Cabin / Aircraft Event  96986 non-null  int64 \n"," 15  Anomaly_Ground Incursion                      96986 non-null  int64 \n"," 16  Anomaly_Airspace Violation                    96986 non-null  int64 \n"," 17  Anomaly_Deviation - Speed                     96986 non-null  int64 \n"," 18  Anomaly_Ground Excursion                      96986 non-null  int64 \n"," 19  Anomaly_No Specific Anomaly Occurred          96986 non-null  int64 \n","dtypes: int64(14), object(6)\n","memory usage: 15.5+ MB\n"]}]},{"cell_type":"code","source":["# Retriece the list of Anomaly label columns\n","Anomaly_RootLabels_columns = []\n","\n","for col in df.columns:\n","  if 'Anomaly_' in str(col):\n","      Anomaly_RootLabels_columns.append(col)"],"metadata":{"id":"Hu1AyD5cC2r9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Anomaly_RootLabels_columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEXninYMC6fL","executionInfo":{"status":"ok","timestamp":1666196558152,"user_tz":-60,"elapsed":5,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"8bc5a3ce-ffb4-417f-8530-bed20c95cbb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Anomaly_Deviation / Discrepancy - Procedural',\n"," 'Anomaly_Aircraft Equipment',\n"," 'Anomaly_Conflict',\n"," 'Anomaly_Inflight Event / Encounter',\n"," 'Anomaly_ATC Issue',\n"," 'Anomaly_Deviation - Altitude',\n"," 'Anomaly_Deviation - Track / Heading',\n"," 'Anomaly_Ground Event / Encounter',\n"," 'Anomaly_Flight Deck / Cabin / Aircraft Event',\n"," 'Anomaly_Ground Incursion',\n"," 'Anomaly_Airspace Violation',\n"," 'Anomaly_Deviation - Speed',\n"," 'Anomaly_Ground Excursion',\n"," 'Anomaly_No Specific Anomaly Occurred']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# PREDICTION"],"metadata":{"id":"0N8bhXPmDGnP"}},{"cell_type":"code","source":["def Naive_bayes_count(data, target, max_features):\n","  \"\"\"\n","  Return the predictions metrics for Naive bayes\n","\n","  Inputs:\n","  - data: pd.Series containing the narratives as single string\n","  - target: ndarray of a shape(#samples, #classes) containing 0's and 1's\n","            A sample may contain several 1's (multilabel context)\n","  - max_features : Consider the top max_features ordered by term frequency across the corpus.\n","\n","  Returns: \n","  - a classification report in a form of dataframe displaing the different metrics \n","    and their values for each label\n","\n","  \"\"\"\n","  \n","  # Train-test split  \n","  X_train, X_test, y_train, y_test = train_test_split(data, target, \n","                                                      test_size= 0.2, \n","                                                      random_state = 12)\n","  \n","  d = pd.DataFrame()\n","  \n","  # Define a pipeline combining a text feature extractor with multi lable classifier\n","  NB_pipeline = Pipeline([\n","      ('vect', CountVectorizer(token_pattern=r\"\\b[a-zA-Z][a-zA-Z]+\\b\",\n","                                  max_features = max_features)),\n","                      ('clf', OneVsRestClassifier(MultinomialNB(\n","                          fit_prior=True, class_prior=None))),\n","                  ])\n","  \n","  # Train the model using X_train and y_train_Anomaly\n","  NB_pipeline.fit(X_train,y_train)\n","  \n","  # Compute the testing accuracy \n","  y_pred = NB_pipeline.predict(X_test)\n","    \n","  # classification report \n","  clf_rep = classification_report(y_test, y_pred, output_dict=True)\n","  \n","  # write classification report dictionnary into pd.DataFrame\n","  metrics = pd.DataFrame(clf_rep)\n","  \n","  # The rest of the code is basically kind of 'transposing' the format \n","  # and adding extra columns with parameter values\n","\n","  # Rename columns with anomaly names\n","  # Crete dictionary with correspondance among label indices and anomaly names\n","  anomaly_labels = dict(zip(metrics.columns[0:14], Anomaly_RootLabels_columns))\n","  metrics = metrics.rename(columns = anomaly_labels)\n","\n","  # Create DataFrame in the right format for the plotting of results\n","  clf_rep_df = pd.DataFrame()\n","  for anomaly in metrics.columns[0:14]:\n","\n","    temp_df = pd.DataFrame(index = metrics.index) # create temporary DataFrame with the 4 metrics as index\n","    temp_df['values'] = metrics.filter(items = [anomaly]).values # write the 4 values for the selected anomaly\n","    temp_df['anomaly'] = anomaly # fill in the column with the selected anomaly label\n","    clf_rep_df = pd.concat([clf_rep_df, temp_df])\n","\n","  clf_rep_df = clf_rep_df.reset_index().rename(columns = {'index': 'metric'})\n","\n","  # Fill in additionnal columns with metadata\n","  clf_rep_df['classifier'] = 'naive bayes'  # [Word_Embedding, BERT...]\n","  clf_rep_df['preprocessing'] = 1              # 0 if raw, 1 if preprocessed\n","  clf_rep_df['undersampling'] = 0 \n","  clf_rep_df['num_words'] = max_features\n","  clf_rep_df['vectorizer'] = 'count'     \n","  \n","  # Reorder columns\n","  clf_rep_df = clf_rep_df[['classifier','vectorizer', 'preprocessing', 'undersampling',\n","                      'anomaly', 'num_words', 'metric', 'values']]\n","  \n","\n","  return clf_rep_df"],"metadata":{"id":"XP4St81vb63O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Naive_bayes_tidf(data, target, max_features):\n","  \"\"\"\n","  Return the predictions metrics for Naive bayes\n","\n","  Inputs:\n","  - data: pd.Series containing the narratives as single string\n","  - target: ndarray of a shape(#samples, #classes) containing 0's and 1's\n","            A sample may contain several 1's (multilabel context)\n","  - max_features : Consider the top max_features ordered by term frequency across the corpus.\n","\n","  Returns: \n","  - a classification report in a form of dataframe displaing the different metrics \n","    and their values for each label\n","\n","  \"\"\"\n","  \n","  # Train-test split  \n","  X_train, X_test, y_train, y_test = train_test_split(data, target, \n","                                                      test_size= 0.2, \n","                                                      random_state = 12)\n","  \n","  d = pd.DataFrame()\n","  \n","  # Define a pipeline combining a text feature extractor with multi lable classifier\n","  NB_pipeline = Pipeline([\n","      ('tidf', TfidfVectorizer(token_pattern=r\"\\b[a-zA-Z][a-zA-Z]+\\b\",\n","                                  max_features = max_features)),\n","                      ('clf', OneVsRestClassifier(MultinomialNB(\n","                          fit_prior=True, class_prior=None))),\n","                  ])\n","  \n","  # Train the model using X_train and y_train_Anomaly\n","  NB_pipeline.fit(X_train,y_train)\n","  \n","  # Compute the testing accuracy \n","  y_pred = NB_pipeline.predict(X_test)\n","    \n","  # classification report \n","  clf_rep = classification_report(y_test, y_pred, output_dict=True)\n","  \n","  # write classification report dictionnary into pd.DataFrame\n","  metrics = pd.DataFrame(clf_rep)\n","  \n","  # The rest of the code is basically kind of 'transposing' the format \n","  # and adding extra columns with parameter values\n","\n","  # Rename columns with anomaly names\n","  # Crete dictionary with correspondance among label indices and anomaly names\n","  anomaly_labels = dict(zip(metrics.columns[0:14], Anomaly_RootLabels_columns))\n","  metrics = metrics.rename(columns = anomaly_labels)\n","\n","  # Create DataFrame in the right format for the plotting of results\n","  clf_rep_df = pd.DataFrame()\n","  for anomaly in metrics.columns[0:14]:\n","\n","    temp_df = pd.DataFrame(index = metrics.index) # create temporary DataFrame with the 4 metrics as index\n","    temp_df['values'] = metrics.filter(items = [anomaly]).values # write the 4 values for the selected anomaly\n","    temp_df['anomaly'] = anomaly # fill in the column with the selected anomaly label\n","    clf_rep_df = pd.concat([clf_rep_df, temp_df])\n","\n","  clf_rep_df = clf_rep_df.reset_index().rename(columns = {'index': 'metric'})\n","\n","  # Fill in additionnal columns with metadata\n","  clf_rep_df['classifier'] = 'naive bayes'  # [Word_Embedding, BERT...]\n","  clf_rep_df['preprocessing'] = 1              # 0 if raw, 1 if preprocessed\n","  clf_rep_df['undersampling'] = 0 \n","  clf_rep_df['num_words'] = max_features\n","  clf_rep_df['vectorizer'] = 'tfidf'     \n","  \n","  # Reorder columns\n","  clf_rep_df = clf_rep_df[['classifier','vectorizer', 'preprocessing', 'undersampling',\n","                      'anomaly', 'num_words', 'metric', 'values']]\n","  \n","\n","  return clf_rep_df"],"metadata":{"id":"nXrJnN3pRlNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize the objects necesseray function inputs\n","data = df['Narrative_PP_stemmed_str']\n","target = df[Anomaly_RootLabels_columns].values\n","\n","#Generate the classification report tabels for both Countvectorizer and Tfidf vectorizer\n","naive_bayes_count_500 = Naive_bayes_count(data, target,max_features = 500)\n","naive_bayes_count_1000 = Naive_bayes_count(data, target,max_features = 1000)\n","naive_bayes_count_3000 = Naive_bayes_count(data, target,max_features = 3000)\n","\n","naive_bayes_tfidf_500 = Naive_bayes_tidf(data, target,max_features = 500)\n","naive_bayes_tfidf_1000 = Naive_bayes_tidf(data, target,max_features = 1000)\n","naive_bayes_tfidf_3000 = Naive_bayes_tidf(data, target,max_features = 3000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zow9qsPoSo1r","executionInfo":{"status":"ok","timestamp":1666276277761,"user_tz":-60,"elapsed":35971,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"71703009-2b49-44b3-cf0f-b74ae756775b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_COUNTVEC_PP_NWORDS_3000.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_count_3000], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dUsaCUUEVKE","executionInfo":{"status":"ok","timestamp":1666276277761,"user_tz":-60,"elapsed":5,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"d7bd2660-8d6f-49a2-8f7b-43b94fdff722"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_COUNTVEC_PP_NWORDS_1000.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_count_1000], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myB4beDQFpui","executionInfo":{"status":"ok","timestamp":1666276277762,"user_tz":-60,"elapsed":3,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"26f683c9-d0e5-4d2d-9fb3-bfe8acad32c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_COUNTVEC_PP_NWORDS_500.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_count_500], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQFYJdy7Fp_y","executionInfo":{"status":"ok","timestamp":1666276277995,"user_tz":-60,"elapsed":235,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"91993997-4b05-4804-ed89-5daf4b6fcba8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_TIDFVEC_PP_NWORDS_500.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_tfidf_500], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XAfRE7rCFqtW","executionInfo":{"status":"ok","timestamp":1666276278260,"user_tz":-60,"elapsed":267,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"ea04e47b-6114-48bf-f2ec-1e4a24cfda24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_TIDFVEC_PP_NWORDS_1000.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_tfidf_1000], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63-rFoaCFq5y","executionInfo":{"status":"ok","timestamp":1666276278514,"user_tz":-60,"elapsed":254,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"acb03777-173c-4b67-98fa-fa8abc1e45ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data/transformed/model_results/\n","filename = 'NBayes_TIDFVEC_PP_NWORDS_3000.pkl'\n","#enregistre le modèle\n","pkl.dump([naive_bayes_tfidf_3000], open(filename, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vX65VXnFrEY","executionInfo":{"status":"ok","timestamp":1665061275954,"user_tz":-60,"elapsed":290,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"d6a2158f-4660-4d76-f915-2632decdd34e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed/model_results\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1qhXdYmJvtunv1bhivEwdLB9bbYKeIMoX","timestamp":1656941850410},{"file_id":"1rtxLNDvVau-UYbcAkUcA0GUgc78nRgIq","timestamp":1656586559783}],"toc_visible":true,"authorship_tag":"ABX9TyNSJFl1wuN7xec5wjcsNvaR"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}