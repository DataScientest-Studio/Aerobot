{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1pL7DYscWMadXk5vN-4lr1DdtFDpZm-o3","timestamp":1660036766808},{"file_id":"1ZEp4JZpDayzHDaMeruxp1F9uV7EjUKBF","timestamp":1659954654096},{"file_id":"1qhXdYmJvtunv1bhivEwdLB9bbYKeIMoX","timestamp":1656941850410},{"file_id":"1rtxLNDvVau-UYbcAkUcA0GUgc78nRgIq","timestamp":1656586559783}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNxAgAcIW48wuc8lVyf4chu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# ABOUT"],"metadata":{"id":"a59UKY4T2lk_"}},{"cell_type":"markdown","source":["\n","Datascientest's Datascientist continuous bootcamp - cohorte Mars2022 -  AeroBOT project\n","\n","**Tutor**\n","\n","* Alban THUET\n","\n","**Authors:**\n","\n","* HÃ©lÃ¨ne ASSIR\n","* Hichem HADJI  \n","* [Ioannis STASINOPOULOS](https://www.linkedin.com/in/ioannis-stasinopoulos/)\n","\n","</br>\n","\n","---\n","</br>\n","\n","**Version History**\n","\n","Version | Date       | Author(s)  | Modification\n","--------|----------- | ---------  | --------------------------\n","4.0     | 23/10/2022 | I.S        | reproduce the training of model 7.3.9.3\n","3.0     | 06/10/2022 | I.S        | global function calling the classes\n","2.0     | 06/10/2022 | I.S        | pipeline with dummy test set\n","1.1     | 30/09/2022 | I.S        | Remove preprocessing part by using `04.1_Anomaly - Feature definition.ipynb`\n","1.0     | 24/09/2022 | I.S        | Document creation"],"metadata":{"id":"DX2lsmPYcVwp"}},{"cell_type":"markdown","source":["This notebook is the 1st try to refactor the code for BERT into a class object.\n","It was copied from 7_3_9_3_UNfrozen_2022_09_14.ipynb\n","We removed the preprocessing part, thanks to its centralisation in the main notebook `04.1_Anomaly - Feature definition.ipynb`"],"metadata":{"id":"vKhxiy5acdjC"}},{"cell_type":"markdown","source":["Now we try to pass a dummy test set trough a saved model, for inference only."],"metadata":{"id":"W064v4RROTea"}},{"cell_type":"markdown","source":["# IMPORT PACKAGES\n"],"metadata":{"id":"tAJ40jap2rBh"}},{"cell_type":"markdown","source":["/!\\ Contains important Deep Learning package imports. \n","Verify before substituting with others\n"],"metadata":{"id":"qH7RERJu3N0Z"}},{"cell_type":"code","source":["#######################\n","# Import packages\n","#######################\n","import numpy as np\n","import seaborn as sns\n","import math # for math.pi etc.\n","import time # time code execution\n","\n","#######################\n","# Pandas\n","#######################\n","import pandas as pd\n","# Set pandas settings to show all data when using .head(), .columns etc.\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None\n","pd.set_option(\"display.colheader_justify\",\"left\") # left-justify the print output of pandas\n","\n","### Display full columnwidth\n","# Set pandas settings to display full text columns\n","#pd.options.display.max_colwidth = None\n","# Restore pandas settings to display standard colwidth\n","pd.reset_option('display.max_colwidth')\n","\n","import itertools # Pour crÃ©er des iterateurs\n","\n","# Package to show the progression of pandas operations\n","from tqdm import tqdm\n","# from tqdm.auto import tqdm  # for notebooks\n","\n","# Create new `pandas` methods which use `tqdm` progress\n","# (can use tqdm_gui, optional kwargs, etc.)\n","tqdm.pandas()\n","# simply use .progress_apply() instead of .apply() on your pd.DataFram\n","\n","######################\n","# PLOTTING\n","######################\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# Define global plot parameters for better readability and consistency among plots\n","# A complete list of the rcParams keys can be retrieved via plt.rcParams.keys() function\n","plt.rcParams['axes.titlesize'] = 30\n","plt.rcParams['axes.labelsize'] = 23\n","plt.rcParams['xtick.labelsize'] = 23\n","plt.rcParams['ytick.labelsize'] = 23\n","plt.rc('legend', fontsize=23)    # legend fontsize\n","\n","# BOKEH \n","from bokeh.plotting import figure # Importation de la classe figure qui permet de crÃ©er un graphique bokeh.\n","from bokeh.io import  push_notebook, output_notebook, show\n","output_notebook() # permet d'afficher tous les futurs graphiques dans l'output d'une cellule jupyter. Si cette instruction n'est pas lancÃ©e, la figure s'affichera dans un nouvel onglet.\n","from bokeh.models import ColumnDataSource\n","from bokeh.transform import dodge\n","from bokeh.models.tools import HoverTool\n","\n","#####################\n","# NLP \n","#####################\n","import re # for Regular Expression handling\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet') # WordNet lemmatizer\n","nltk.download('omw-1.4') # necessary for WordNet lemmatizer\n","from nltk.tokenize import word_tokenize # Usual tokenizer\n","from nltk.tokenize import TweetTokenizer # Special tokenizer;  \"we'll\", \"didn't\", etc. are considered as one word\n","from sklearn.feature_extraction.text import CountVectorizer # Vectorization\n","from nltk.corpus import stopwords # Import stopwords from nltk.corpus\n","from nltk.stem.snowball import EnglishStemmer\n","\n","###############################\n","# ML preprocessing and models\n","###############################\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import ensemble # random forest\n","from sklearn.svm import SVC\n","\n","# EVALUATION tools from sklearn\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import roc_curve, auc, multilabel_confusion_matrix, average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n","\n","###############################\n","# Deep Learning\n","###############################\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Layer, Input, Dense, Embedding, Flatten, Dropout, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras import callbacks\n","\n","\n","###############################\n","# Other\n","###############################\n","import pickle as pkl # Saving data externally"],"metadata":{"id":"5_efzqI3_FPo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666526133478,"user_tz":-120,"elapsed":4882,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"688869cd-1ef8-4dae-ada4-1c47a9a11fe7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["# IMPORTANT: LOAD FUNCTIONS\n","\n","Functions common to several sections of this code (we should actually put them in a separate file and load them from there)"],"metadata":{"id":"AQEjc1N_aX2V"}},{"cell_type":"code","source":["def plot_train_history(training_history, metric, anomaly_name):\n","  \"\"\"\n","  Generete plots to monitor the train process\n","  Inputs: \n","  - 'training_history'; use training_history = model.train(...)\n","  - 'metric' to plot; string e.g. 'accuracy', 'loss'\n","  - 'anomaly_name' e.g. 'Anomaly_Conflict'. This is used for the plot title\n","  \"\"\"\n","  fig = plt.figure(figsize = (10,4))\n","  #plt.title(f\"{anomaly_name} train history - {metric.upper()}\", fontsize = 20)\n","  train_acc = training_history.history[metric]\n","  val_acc = training_history.history['val_' + metric] # e.g. 'val_accuracy'\n","\n","  plt.plot(train_acc, label = f'Training {metric}')\n","  plt.plot(val_acc, label = f'Validation {metric}')\n","  plt.xlabel('epochs')\n","  plt.ylabel(f'{metric}')\n","  plt.legend()\n","  plt.show();"],"metadata":{"id":"O9VX-lX3aiCq","executionInfo":{"status":"ok","timestamp":1666526133481,"user_tz":-120,"elapsed":17,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def y_prob_to_y_pred_ML(y_pred_proba, threshold = 0.5):\n","  \"\"\"\n","  Converts probabilities into 0's and 1's. We are still in the MULTILABEL context.\n","  Input: MULTILABEL predictions (probabilities whose sum for each sample may exceed > 1) coming directly from the model\n","  Using a user-defined threshold, return a MULTILABEL prediction vector 'y_pred' containing 0's and 1's\n","  \"\"\"\n","  y_pred=[]\n","  for sample in y_pred_proba:\n","    y_pred.append([1 if i>= threshold else 0 for i in sample])\n","  y_pred = np.array(y_pred)\n","\n","  return y_pred"],"metadata":{"id":"vYh1tO4l6AGg","executionInfo":{"status":"ok","timestamp":1666526133482,"user_tz":-120,"elapsed":17,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def create_dir_if_not_exists(path):\n","  \"\"\"\n","  Check if the directory 'path' exists and create if necessary.\n","  \"\"\"\n","  import os\n","  if not os.path.exists(path):\n","    # Create a new directory because it does not exist\n","    os.makedirs(path)\n","    print(f\"New directory created:\\n {path} \\n\")"],"metadata":{"id":"MpQwgNm3lKdg","executionInfo":{"status":"ok","timestamp":1666526133482,"user_tz":-120,"elapsed":17,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def save_ML_outputs(dir_name, experiment_name, **kwargs):\n","    \"\"\"\n","    Save multilabel classification outputs (y_pred_proba, y_pred, clf_rep) and target\n","    variable y_test.\n","    Uses **kwargs, so that the user may save only a part of the variables.\n","\n","    Inputs\n","    -------\n","    - 'dir_name' (str): root directory, with slash at the end\n","    - 'experiment_name' (str): subdirectory, appended to 'dir_name'. Is also\n","      part of the .pkl file's name.\n","    - additional keyword arguments:\n","      - y_pred_proba\n","      - y_pred\n","      - y_test\n","      - 'clf_rep' (classification report in \n","      dictionary format)\n","      - 'clf_rep_df' (classification report in \n","      pd.DataFrame format)\n","    \"\"\"\n","    create_dir_if_not_exists(dir_name)\n","\n","    # 'Unpack' the optional keyword arguments\n","    # kwargs behaves like a dictionary\n","    for key, val in zip(kwargs.keys(), kwargs.values()):\n","      print(\"Saving\", str(key), \"...\")\n","      filename_pkl = experiment_name + '_' + str(key) +  '.pkl'\n","      path_and_filename_pkl = dir_name + filename_pkl\n","      pkl.dump(val, open(path_and_filename_pkl, 'wb'))\n","\n","    print(\"\\n\")\n","    print(\"Multilabel results were successfully saved in \\n\", dir_name)"],"metadata":{"id":"pqRLPczHTFuQ","executionInfo":{"status":"ok","timestamp":1666526133483,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def convert_clf_rep_to_df_multilabel_BERT_kw_args(clf_rep, anomalies, **kwargs):\n","  '''\n","  Return the classification report in form of a pd.DataFrame.\n","  Tailored for extracting MULTILABEL BERT experiment results.\n","  The DataFrame contains dditional columns containing experiment information.\n","  Improvement: Their construction should be automatised as much as possible.\n","  \n","  Inputs\n","  -------\n","  - multilabel classification report in dictionary format\n","  (does not contain '0' and '1' keys)\n","  - anomalies (list of str): Used to label the classification report.\n","    Best use the .anomalies attribute of a 'DataPrepMultilabelBERT' class \n","    object.\n","  - additional keyword arguments. Pass the ones you wish and even additional ones.\n","    They are all automatically unpacked, a column is created in the DataFrame, with \n","    col_name = str(keyword_of_the_argument) and value = kwarg_value.\n","    \n","    Here is a nonexhaustive list of possible kwargs in the context of our \n","    transformers:\n","    - classifier (str), e.g. 'BERT_BASE' or 'DistilBERT' \n","    - preprocessing (str), e.g. 'original' or 'raw_stem' or 'PP'. 'original' in AeroBot\n","      means no preprocessing at all, not even tokenization or stemming\n","    - undersampling (int), 0 or 1 if undersampling was applied\n","\n","    - UNfrozen_layers (str), e.g. '9_10_11_12' if the last 4 layers were trained, \n","      str(None) if all layers were frozen. \n","      You can pass 'str(model.trainable_layers)'.\n","      We use str() in case the value is 'None', other wise it is just empty.\n","      Layers run from 1 to 12!\n","    - concat_layers (str), whether / which layers were concatenated, e.g. str(None) \n","      or '8_9_10_11'\n","      You can pass 'str(trained_transformer_model.concat_slice.layers_to_concat)'\n","      We use str() in case the value is 'None', other wise it is just empty.\n","      Layers run from 1 to 12!\n","    - comments (str), misc. comment, e.g. 'last_hidden_state_CLS_random_state_222'\n","      or 'Flatten layer X' or 'max_length_345' or 'last_hidden_state_CLS'\n","    - experiment_ID (str), e.g. '7_3_9_4'\n","    - padding (str), padding setting, e.g. 'pre' or 'post'\n","    - truncating (str), truncation setting, e.g. 'pre' or 'post'\n","    - maxlen (int), max_length value (length of tokenized sequence)   \n","\n","  Return\n","  -------\n","  - classification report in form of a pd.DataFrame with additional columns \n","    containing experiment information\n","  '''\n","  # write classification report dictionnary into pd.DataFrame\n","  metrics = pd.DataFrame(clf_rep)\n","\n","  # The rest of the code is basically kind of 'transposing' the format \n","  # and adding extra columns with parameter values\n","\n","  # Rename columns with anomaly names\n","  # Crete dictionary with correspondance among label indices and anomaly names\n","  anomaly_labels = dict(zip(metrics.columns[0:len(anomalies)], anomalies))\n","  metrics = metrics.rename(columns = anomaly_labels)\n","\n","  ##########################################################\n","  # Create DataFrame in the right format for the plotting of results\n","  clf_rep_df = pd.DataFrame()\n","  for anomaly in metrics.columns[0:len(anomalies)]:\n","\n","    temp_df = pd.DataFrame(index = metrics.index) # create temporary DataFrame with the 4 metrics as index\n","    temp_df['values'] = metrics.filter(items = [anomaly]).values # write the 4 values for the selected anomaly\n","    temp_df['anomaly'] = anomaly # fill in the column with the selected anomaly label\n","    clf_rep_df = pd.concat([clf_rep_df, temp_df])\n","\n","  clf_rep_df = clf_rep_df.reset_index().rename(columns = {'index': 'metric'})\n","\n","  # Fill in additionnal columns with metadata by 'unpacking' the \n","  # keyword arguments\n","  for key, val in zip(kwargs.keys(), kwargs.values()):\n","    clf_rep_df[str(key)] = val        # 'BERT_BASE' or 'DistilBERT'\n","\n","  # # Reorder columns DO WE REALLY NEED TO ? CHECK 6.3.4 (import - plotting notebook)\n","  # clf_rep_df = clf_rep_df[[\\\n","  #                          'experiment_ID',\n","  #                          'classifier', \n","  #                          'preprocessing', \n","  #                          'undersampling',\n","  #                          'UNfrozen_layers',\n","  #                          'concat_layers',\n","  #                          'comments',\n","  #                          'anomaly', \n","  #                          #'num_words', \n","  #                          #'maxlen', \n","  #                          #'padding', \n","  #                          #'truncating', \n","  #                          'metric', \n","  #                          'values']]\n","  print(\"Classification report successfully converted into DataFrame of length:\", len(clf_rep_df)) #should be 56 = 14 anomalies * 4 metrics\n","\n","  return clf_rep_df"],"metadata":{"id":"GdYthe8sTSqM","executionInfo":{"status":"ok","timestamp":1666526133483,"user_tz":-120,"elapsed":14,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def save_exp_info_to_txt(dir_name, experiment_name, model_attr_to_save, \n","                         class_objects, model, include_model_summary = True,\n","                         **kwargs):\n","    \"\"\"\n","    Save experiment information to a .txt file for future reference.\n","\n","    Inputs\n","    -------\n","    - 'dir_name' (str): root directory, with slash at the end\n","    - 'experiment_name' (str): subdirectory, appended to 'dir_name'. Is also\n","      part of the .txt file's name.\n","    - model_attr_to_save (list of strings): model attributes to save, e.g. \n","      'BERT_model_name', 'trainable_layers', 'num_classes', 'anomalies', \n","      'batch_size','max_length', ...\n","    - class_objects (list of class objects): class objects, where to look for the \n","      'model_attr_to_save'\n","    - model (transformer model object): used to generate the model.summary() and\n","      write it to the file  \n","    - include_model_summary (bool) whether to include the model summary in the \n","      .txt file. Default = True\n","    - additional keyword arguments that should be saved in the .txt file\n","\n","    Return\n","    -------\n","    None; creates a .txt file in directory 'dir_name'.\n","    \"\"\"\n","    create_dir_if_not_exists(dir_name)\n","    \n","    ###################################################################\n","    # PREPARE THE DATA TO BE SAVED INTO A DICTIONARY 'dict_for_export'\n","    ###################################################################\n","    # Initialize empty dictionary\n","    dict_for_export = dict({})\n","\n","    # Loop through the class objects\n","    for obj in class_objects:\n","      # Get the object's attributes in form of a dictionary\n","      d = obj.__dict__\n","      # Select only the elements whose key is in 'model_attr_to_save'\n","      param_dict = {k:d[k] for k in model_attr_to_save if k in d}\n","      # 'Append' to the dictionary 'dict_for_export'\n","      dict_for_export.update(param_dict) # if two keys are the same (e.g. 'max_length'), it overwrites the value\n","\n","    # Append additional elements that are not class attributes, passed via the \n","    # kwargs\n","    for key, val in zip(kwargs.keys(), kwargs.values()):\n","      dict_for_export.update({str(key):val})\n","\n","    ###########################\n","    # WRITE DATA TO .txt file\n","    ###########################\n","    # Write dictionary to .txt file\n","    filename = dir_name + experiment_name + '_exp_info.txt'\n","    with open(filename, 'w') as f:\n","        print(dict_for_export, file = f)\n","\n","    if include_model_summary == True:\n","      # Append the model summary to the .txt file\n","      with open(filename, 'a') as f: # 'a' stands for append; prevents overwritting\n","      # print(dict_for_export, file = f)\n","        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n","\n","    print(\"Experiment information successfully written to .txt file located in:\\n\", dir_name)"],"metadata":{"id":"fddzMlqsMbg1","executionInfo":{"status":"ok","timestamp":1666526133484,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# LOAD DATA"],"metadata":{"id":"cjRrSdqQ7llR"}},{"cell_type":"markdown","source":["## Mount GDrive"],"metadata":{"id":"BfmhmLv0_Osg"}},{"cell_type":"code","source":["#@title\n","# Mount your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#check your present working directory \n","%pwd"],"metadata":{"id":"N_mjKklM_bJH","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1666526149166,"user_tz":-120,"elapsed":15697,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"7fd5d5ba-06b4-4a16-8b66-2f36dac0be91"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#@title\n","# move to the transformed data location (you can create a deeper structure, if needed, e.g. to save a trained model):\n","%cd /content/drive/MyDrive/data/transformed/"],"metadata":{"id":"S72xAGPS_bGM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666526149167,"user_tz":-120,"elapsed":12,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"11d6f0cb-eb12-4294-e2b9-4eeecfccb728","cellView":"form"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed\n"]}]},{"cell_type":"code","source":["#@title\n","!ls # list the content of the pwd\n","\n","#!ls \"/content/drive/MyDrive/Data_Science/Formations/DataScienceTest/projet/AeroBot/\" # list contect of a speficic folder"],"metadata":{"id":"NkiTSU2c_bDh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666526149499,"user_tz":-120,"elapsed":339,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"a271d072-4bd9-4199-f436-884a9b7b1de1","cellView":"form"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":[" 2022_09_11_7_4_3_raw_narr_BERT_BASE_frozen_max_length_345.pkl\n"," complaints-2022-08-05_13_55.csv\n","'Copy of Qualified abbreviations_20220718.xlsx.gsheet'\n","'Data Dictionnary.xlsx'\n"," data_for_BERT_multilabel_20220805.pkl\n"," df_for_Anomaly_prediction.pkl\n"," df_test_for_Anomaly_prediction.pkl\n"," model.png\n"," model_results\n"," Narrative_PP_stemmed_24072022_TRAIN.pkl\n"," Narrative_Raw_Stemmed_24072022_TRAIN.pkl\n"," Narrative_RegEx_subst_21072022_TRAIN.pkl\n","'Qualified abbreviations_20220707_test.csv'\n","'Qualified abbreviations_20220708.csv'\n","'Qualified abbreviations_20220718.csv'\n","'Qualified abbreviations_20220718_Google_sheet.gsheet'\n"," test_data_final.pkl\n"," train_data_final.pkl\n"]}]},{"cell_type":"markdown","source":["## Load data from .pkl file\n"],"metadata":{"id":"pB8SGMTD4JsN"}},{"cell_type":"code","source":["# # Load the TRAIN data (97417 entries)\n","# # Do not touch the TEST data until the end of the project!\n","# # or the curse of the greek gods will fall upon you!\n","\n","# %cd /content/drive/MyDrive/data/transformed/\n","# with open(\"df_for_Anomaly_prediction.pkl\", \"rb\") as f:\n","#     loaded_data = pkl.load(f)\n","\n","# df = loaded_data\n","# print(\"\\nA Dataframe with\", len(df), \"entries has been loaded\")"],"metadata":{"id":"Mq5adcXe9Z22","executionInfo":{"status":"ok","timestamp":1666526149499,"user_tz":-120,"elapsed":7,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Load the FINAL TEST data (10805 entries)\n","# Do not touch the TEST data until the end of the project!\n","# or the curse of the greek gods will fall upon you!\n","\n","%cd /content/drive/MyDrive/data/transformed/\n","with open(\"df_test_for_Anomaly_prediction.pkl\", \"rb\") as f:\n","    loaded_data = pkl.load(f)\n","\n","df = loaded_data\n","print(\"\\nA Dataframe with\", len(df), \"entries has been loaded\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z67xez2cVqVD","executionInfo":{"status":"ok","timestamp":1666526149771,"user_tz":-120,"elapsed":278,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"1633feaa-9161-4180-bbb3-854153003823"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/transformed\n","\n","A Dataframe with 10805 entries has been loaded\n"]}]},{"cell_type":"code","source":["df.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"alJykjhuTphb","executionInfo":{"status":"ok","timestamp":1666526149772,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"33baec59-426a-4ca2-d10f-d515a2cceca3"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        Narrative                                           \\\n","ACN                                                          \n","1014798  Flying into SLC on the DELTA THREE RNAV arriva...   \n","1806744  ORD was on a very busy east flow arrival push....   \n","\n","        Anomaly                                             \\\n","ACN                                                          \n","1014798  Aircraft Equipment Problem Less Severe; Deviat...   \n","1806744  ATC Issue All Types; Conflict NMAC; Deviation ...   \n","\n","         Anomaly_Deviation / Discrepancy - Procedural  \\\n","ACN                                                     \n","1014798  1                                              \n","1806744  1                                              \n","\n","         Anomaly_Aircraft Equipment  Anomaly_Conflict  \\\n","ACN                                                     \n","1014798  1                           0                  \n","1806744  0                           1                  \n","\n","         Anomaly_Inflight Event / Encounter  Anomaly_ATC Issue  \\\n","ACN                                                              \n","1014798  0                                   0                   \n","1806744  0                                   1                   \n","\n","         Anomaly_Deviation - Altitude  Anomaly_Deviation - Track / Heading  \\\n","ACN                                                                          \n","1014798  1                             0                                     \n","1806744  0                             0                                     \n","\n","         Anomaly_Ground Event / Encounter  \\\n","ACN                                         \n","1014798  0                                  \n","1806744  0                                  \n","\n","         Anomaly_Flight Deck / Cabin / Aircraft Event  \\\n","ACN                                                     \n","1014798  0                                              \n","1806744  0                                              \n","\n","         Anomaly_Ground Incursion  Anomaly_Airspace Violation  \\\n","ACN                                                             \n","1014798  0                         0                            \n","1806744  0                         0                            \n","\n","         Anomaly_Deviation - Speed  Anomaly_Ground Excursion  \\\n","ACN                                                            \n","1014798  0                          0                          \n","1806744  0                          0                          \n","\n","         Anomaly_No Specific Anomaly Occurred  \n","ACN                                            \n","1014798  0                                     \n","1806744  0                                     "],"text/html":["\n","  <div id=\"df-fa90bf9b-4bba-4795-81bc-cbda250f9b0e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th></th>\n","      <th>Narrative</th>\n","      <th>Anomaly</th>\n","      <th>Anomaly_Deviation / Discrepancy - Procedural</th>\n","      <th>Anomaly_Aircraft Equipment</th>\n","      <th>Anomaly_Conflict</th>\n","      <th>Anomaly_Inflight Event / Encounter</th>\n","      <th>Anomaly_ATC Issue</th>\n","      <th>Anomaly_Deviation - Altitude</th>\n","      <th>Anomaly_Deviation - Track / Heading</th>\n","      <th>Anomaly_Ground Event / Encounter</th>\n","      <th>Anomaly_Flight Deck / Cabin / Aircraft Event</th>\n","      <th>Anomaly_Ground Incursion</th>\n","      <th>Anomaly_Airspace Violation</th>\n","      <th>Anomaly_Deviation - Speed</th>\n","      <th>Anomaly_Ground Excursion</th>\n","      <th>Anomaly_No Specific Anomaly Occurred</th>\n","    </tr>\n","    <tr>\n","      <th>ACN</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1014798</th>\n","      <td>Flying into SLC on the DELTA THREE RNAV arriva...</td>\n","      <td>Aircraft Equipment Problem Less Severe; Deviat...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1806744</th>\n","      <td>ORD was on a very busy east flow arrival push....</td>\n","      <td>ATC Issue All Types; Conflict NMAC; Deviation ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa90bf9b-4bba-4795-81bc-cbda250f9b0e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fa90bf9b-4bba-4795-81bc-cbda250f9b0e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fa90bf9b-4bba-4795-81bc-cbda250f9b0e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Define Anomaly_RootLabels_columns list from data set"],"metadata":{"id":"Nrx4wTX89hiL"}},{"cell_type":"code","source":["# Retriece the list of Anomaly label columns\n","Anomaly_RootLabels_columns = []\n","\n","for col in df.columns:\n","  if 'Anomaly_' in str(col):\n","      Anomaly_RootLabels_columns.append(col)"],"metadata":{"id":"fZDLXSa69fnJ","executionInfo":{"status":"ok","timestamp":1666526149776,"user_tz":-120,"elapsed":18,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["Anomaly_RootLabels_columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Uw1eBz59mgK","executionInfo":{"status":"ok","timestamp":1666526149777,"user_tz":-120,"elapsed":18,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}},"outputId":"37e281d6-863d-4331-9ca6-f15593c4601c"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Anomaly_Deviation / Discrepancy - Procedural',\n"," 'Anomaly_Aircraft Equipment',\n"," 'Anomaly_Conflict',\n"," 'Anomaly_Inflight Event / Encounter',\n"," 'Anomaly_ATC Issue',\n"," 'Anomaly_Deviation - Altitude',\n"," 'Anomaly_Deviation - Track / Heading',\n"," 'Anomaly_Ground Event / Encounter',\n"," 'Anomaly_Flight Deck / Cabin / Aircraft Event',\n"," 'Anomaly_Ground Incursion',\n"," 'Anomaly_Airspace Violation',\n"," 'Anomaly_Deviation - Speed',\n"," 'Anomaly_Ground Excursion',\n"," 'Anomaly_No Specific Anomaly Occurred']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Multilabel with BERT\n"],"metadata":{"id":"UTxOIAev1MeJ"}},{"cell_type":"markdown","source":["## Install ðŸ¤— Hugging Face "],"metadata":{"id":"D4eGwHCYBjRh"}},{"cell_type":"code","source":["! pip install transformers \n","\n","#! pip install datasets\n","# Use this instead (see https://github.com/huggingface/datasets/pull/5120): \n","! pip install git+https://github.com/huggingface/datasets#egg=datasets\n","\n","! pip install huggingface_hub"],"metadata":{"id":"DQriyJNQBmSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CLASS DEFINITIONS"],"metadata":{"id":"JRxef8KCCFcg"}},{"cell_type":"markdown","source":["### DataPrepMultilabelBERT"],"metadata":{"id":"KWmJp1kYBtAI"}},{"cell_type":"code","source":["class DataPrepMultilabelBERT():\n","  '''\n","  Prepare data for multilabel classification using BERT.\n","  An object of this class will return 3 'tf.data.Dataset' datasets, when called:\n","  tf_train_dataset, tf_validation_dataset, tf_test_dataset.\n","\n","  Inference only\n","  ---------------\n","  If the 'train_mode = True' option is passed when calling an object of this \n","  class, *dummy* train and validation datasets are built, containing only the \n","  first two entries of df (see attributes at instantiation). In this case, \n","  df should equal the test set, on which inference should be performed. \n","  The test set returned contains the full input data 'df'. \n","  For more details, read the DocString of the build_dummy_datasets() method.\n","\n","  Methods\n","  ----------\n","  - __init__(), like every class\n","  - get_text_and_labels()\n","  - train_validation_test_split()\n","  - build_dummy_datasets()\n","  - convert_to_HF_dataset()\n","  - tokenize_the_BERT_way()\n","  - convert_to_TensorFlow_dataset()\n","  - __call__() \n","    Reminder: The __call__() method is called by executing 'object()', \n","    where 'object' is an instance of the class.\n","\n","  Attributes at class object instantiation\n","  -----------------------------------------\n","  Attributes that are created within __call__(). They are the outputs of the \n","  class's own methods.\n","  - BERT_model_name : string, \"google/bert_uncased_L-12_H-768_A-12\" for BERT BASE, \n","    or one of the Small BERTs: \"google/bert_uncased_L-12_H-128_A-2\" \n","    or #google/bert_uncased_L-2_H-128_A-2\" (BERT tiny) \n","    https://huggingface.co/google/bert_uncased_L-12_H-768_A-12\n","  - df: pd.DataFrame, containing \n","      - ACN number (unique code for each ASRS database entry)\n","      - narratives\n","      - one-hot encoded labels for the narratives, e.g. Anomaly\n","  - anomalies: list of anomaly labels, e.g. as obtained from df.columns\n","  - text_input: string, which column to use as the text, \n","    e.g. 'Narrative', 'Narrative_PP_stemmed_str', ...\n","  - max_length: int, default = 200, length of tokenized text \n","  - batch_size: int, default = 32, batch size of the tf.data.Dataset created\n","  - random_state : for the train_validation_test_split() method of the class, \n","    see corresponding Docstring\n","  - first_split_prop : for the train_validation_test_split() method of the class, \n","    see corresponding Docstring\n","  - second_split_prop : for the train_validation_test_split() method of the class, \n","    see corresponding Docstring\n","\n","  Attributes created by the class's __call__() method\n","  ------------------------------------------------\n","  - text_and_labels: output of get_text_and_labels() method\n","  - df_train, df_validation, df_test: outputs of either\n","    - train_validation_test_split() method, or\n","    - build_dummy_datasets() method (when in inference mode)\n","  - HF_dataset, train_dataset_HF, validation_dataset_HF, test_dataset_HF \n","    outputs of convert_to_HF_dataset() method\n","  - tokenized_dataset, train_set_len: outputs of tokenize_the_BERT_way() method\n","  - tf_train_dataset, tf_validation_dataset, tf_test_dataset: outputs of\n","    convert_to_TensorFlow_datasets() method\n","  '''\n","  def __init__(self, \n","               BERT_model_name, \n","               df, \n","               anomalies, \n","               text_input = 'Narrative', \n","               max_length = 200,\n","               batch_size = 32,\n","               random_state = 12, \n","               first_split_prop = 0.2, \n","               second_split_prop = 0.2):\n","\n","    self.BERT_model_name = BERT_model_name\n","    self.df = df\n","    self.anomalies = anomalies\n","    self.text_input = text_input\n","    self.max_length = max_length\n","    self.batch_size = batch_size\n","    self.random_state = random_state\n","    self.first_split_prop = first_split_prop \n","    self.second_split_prop = second_split_prop\n","\n","    # Instantiate a data_collator used by the method 'convert_to_TensorFlow_datasets()'\n","    from transformers import DefaultDataCollator\n","    self.data_collator = DefaultDataCollator(return_tensors=\"tf\")\n","\n","  def get_text_and_labels(self, df, anomalies, text_input):\n","    '''\n","    Return a DataFrame containing a list of binary multilabels and a text per row.\n","    Inputs\n","    ------\n","    - df: pd.DataFrame\n","    - anomalies: list of Anomaly label names\n","    - text_input: name of the column of df that contains the texts, e.g. 'Narrative'\n","    '''\n","    print(\"Creating multilabels...\")\n","    text_and_labels = df[anomalies]\n","    text_and_labels['labels']  = text_and_labels.apply(lambda r: tuple(r), axis=1).apply(np.array)\n","    # /!\\ has to be 'labels' and not 'label' or other. HuggingFace expects this. \n","    # With 'label', I got AssertError in the conversion to tf.dataset\n","    text_and_labels = text_and_labels.drop(columns = self.anomalies)\n","    text_and_labels['text'] = df[self.text_input]\n","    text_and_labels = text_and_labels.reset_index().drop(columns = ['ACN'])\n","    print(\"Example of text and corresponding multilabel:\\n\")\n","    print(text_and_labels.iloc[0])\n","    print(\"\\n\")\n","    print(\"get_text_and_labels() done\")\n","    print(30*\"*\", \"\\n\")\n","    return text_and_labels\n","\n","\n","  def train_validation_test_split(self, \n","                                  text_and_labels, \n","                                  first_split_prop, \n","                                  second_split_prop,\n","                                  random_state):\n","    '''\n","    Split arrays or matrices into random train, validation and test subsets.\n","    First separate test set from df, then separate validation set.\n","\n","    Inputs\n","    ----------\n","    - text_and_labels : pd.DataFrame containing only the columns 'text' and 'label'\n","    - first_split_prop : float, default = 0.2 \n","      Proportion to use in the first split.\n","    - second_split_prop : float, default = 0.2\n","      Proportion to use in the second split.\n","    - random_state : default = 12\n","\n","    Return\n","    ----------\n","    pd.DataFrames\n","    - df_train\n","    - df_validation\n","    - df_test\n","    '''\n","    print(\"Splitting dataset...\")\n","    # Train-test split  \n","    df_train_plus_val, df_test, = train_test_split(text_and_labels, \n","                                                    test_size= first_split_prop,\n","                                                    random_state = random_state) \n","    # Train-validation split \n","    df_train, df_validation, = train_test_split(df_train_plus_val, \n","                                                    test_size= second_split_prop,\n","                                                    random_state = random_state) \n","\n","    print(\"train set length:\", len(df_train))\n","    print(\"validation set length:\", len(df_validation))\n","    print(\"test set length:\", len(df_test))\n","    \n","    print(\"\\n\")\n","    print(\"train_validation_test_split() done\")\n","    print(30*\"*\", \"\\n\")\n","    return df_train, df_validation, df_test\n","\n","  def build_dummy_datasets(self, \n","                           text_and_labels):\n","    '''\n","    Build dummy train and validation datasets, containing only the first two \n","    entries of text_and_labels, which should correspond to the test set.\n","    \n","    The test set returned contains the full input data 'text_and_labels'. \n","    \n","    Why create dummy datasets? In order to modify the data preparation pipeline \n","    as little as possible, because it builds complex data structures \n","    (i.e. HuggingFace dictionary, tf.Data.dataset).\n","\n","    Inputs\n","    ----------\n","    - text_and_labels : pd.DataFrame containing only the columns 'text' and 'label'\n","\n","    Return\n","    ----------\n","    pd.DataFrames\n","    - df_train_dummy\n","    - df_validation_dummy\n","    - df_test\n","    '''\n","    print(\"Building dummy train and validation datasets: they contain only the first two entries of the test set.\")\n","    df_train_dummy = pd.DataFrame(text_and_labels.iloc[:2])\n","    df_validation_dummy = pd.DataFrame(text_and_labels.iloc[:2])\n","    df_test = text_and_labels\n","\n","    print(\"dummy train set length:\", len(df_train_dummy))\n","    print(\"dummy validation set length:\", len(df_validation_dummy))\n","    print(\"test set length:\", len(df_test))\n","    \n","    print(\"\\n\")\n","    print(\"build_dummy_datasets() done\")\n","    print(30*\"*\", \"\\n\")\n","    return df_train_dummy, df_validation_dummy, df_test\n","    \n","\n","  def convert_to_HF_dataset(self, df_train, df_validation, df_test):\n","    '''\n","    Convert pd.DataFrames to ðŸ¤— datasets and combining them into a ðŸ¤— DatasetDict.\n","    \n","    Input\n","    -------\n","    pd.DataFrames\n","    - df_train\n","    - df_validation\n","    - df_test\n","\n","    Return\n","    -------\n","    - HF_dataset: ðŸ¤— DatasetDict\n","    - Its three constituent ðŸ¤— datasets: \n","      - train_dataset_HF\n","      - validation_dataset_HF\n","      - test_dataset_HF\n","    '''\n","    print(\"Combining pd.DataFrames into a HuggingFace dataset...\")\n","    # IMPORT ðŸ¤— PACKAGES\n","    # when putting outside this function, it did not recognize 'Dataset'\n","    from datasets import Dataset, DatasetDict\n","    \n","    # Convert into ðŸ¤— datasets  \n","    train_dataset_HF = Dataset.from_dict(df_train)\n","    validation_dataset_HF = Dataset.from_dict(df_validation)\n","    test_dataset_HF = Dataset.from_dict(df_test)\n","    # use from_dict() and not from_pandas()\n","    # otherwise you get an extra key, smth litke '__index col__'\n","    print(\"\\n Structure of Hugging Face dataset (train):\", train_dataset_HF)\n","    \n","    # 'merge' the three ðŸ¤— datasets into a single ðŸ¤— DatasetDict\n","    HF_dataset = DatasetDict({\"train\": train_dataset_HF, \n","                              \"validation\": validation_dataset_HF, \n","                              \"test\": test_dataset_HF})\n","    print(\"\\n Structure of the complete Hugging Face dataset:\\n\", HF_dataset)\n","    print(\"\\n First entry of the train dataset:\\n\", HF_dataset[\"train\"][0])\n","    print(\"\\n\")\n","    print(\"convert_to_HF_dataset() done\")\n","    print(30*\"*\", \"\\n\")\n","    return HF_dataset, train_dataset_HF, validation_dataset_HF, test_dataset_HF\n","\n","\n","  def tokenize_the_BERT_way(self, HF_dataset, BERT_model_name, max_length):\n","    '''\n","    Tokenize a ðŸ¤— dataset with the appropriate BERT tokenizer, downloaded from ðŸ¤—\n","\n","    Inputs\n","    ------\n","    - HF_dataset: ðŸ¤— DatasetDict\n","    - BERT_model_name (string), as defined on the ðŸ¤— website\n","    - max_length (int), desired length of sequence \n","    \n","    Return\n","    ------\n","    - tokenized_dataset, a ðŸ¤— DatasetDict\n","    - train_set_len (int): length of the train set\n","    '''\n","    # instantiate tokenizer\n","    # IMPORT ðŸ¤— PACKAGES\n","    from transformers import AutoTokenizer\n","    # TOKENIZE using a pre-trained tokenizer from HuggingFace\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(BERT_model_name)\n","    max_length = max_length\n","\n","    # Define a function in order to use .map() below\n","    def tokenize_function(examples):\n","      return tokenizer(examples[\"text\"], \n","                       padding='max_length', max_length = max_length, # same value as we used for WordEmbedding \n","                       truncation = True\n","                       #return_tensors=\"tf\"\n","                       )\n","    \n","    # Map the tokenization function onto our dataset\n","    # it is ok to apply it also to the test set, since it is a *pretrained* tokenizer,\n","    # i.e. it will not train on our data\n","    print(\"Performing tokenization in batches on: train, validation, test sets...\")\n","\n","    # Auxialiary variable, see below\n","    pre_tokenizer_columns = set(HF_dataset[\"train\"].features)\n","    \n","    tokenized_dataset = HF_dataset.map(tokenize_function, batched = True)\n","    # The tokenizer uses its own batch_size, not the class attribute\n","\n","    # Display the additional columns created by the tokenizer\n","    # They are necessary inputs for BERT\n","    tokenizer_columns = list(set(tokenized_dataset[\"train\"].features) - pre_tokenizer_columns)\n","    print(\"Columns added by tokenizer:\", tokenizer_columns)\n","    print(\"\\n Structure of the complete tokenized Hugging Face dataset:\\n\", tokenized_dataset)\n","    print(\"\\n\")\n","    print(\"tokenize_the_BERT_way() done\")\n","    print(30*\"*\", \"\\n\")\n","    # return length of tokenized_dataset[\"train\"] to feed it to AdamW optimizer\n","    train_set_len = len(tokenized_dataset[\"train\"])\n","\n","    return tokenized_dataset, train_set_len\n","\n","  def convert_to_TensorFlow_datasets(self, tokenized_dataset_to_convert, key =\"test\"):\n","    \"\"\"\n","    Convert our datasets to tf.data.Dataset, which Keras understands natively. \n","\n","    Inputs\n","    -------\n","    - tokenized_dataset_to_convert: a ðŸ¤— DatasetDict\n","    - key (str): key of the ðŸ¤— DatasetDict \"train\", \"validation\", \"test\"\n","\n","    Return\n","    -------\n","    - tf_dataset: tf.data.Dataset\n","    \"\"\"\n","    # Two ways to do the conversion: \n","    # (i) use the slightly more low-level Dataset.to_tf_dataset() method\n","    # (ii) use Model.prepare_tf_dataset(). \n","    # The Model method can inspect the model to determine which column names it \n","    # can use as input, which means you don't need to specify them yourself. \n","    # Unless our samples are all the same length, we will also need to pass \n","    # a tokenizer or collate_fn so that the tf.data.Dataset knows \n","    # how to pad and combine samples into a batch.\n","    tf_dataset = tokenized_dataset_to_convert[key].to_tf_dataset(\n","        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","        label_cols=[\"labels\"],\n","        shuffle=False,\n","        collate_fn = self.data_collator,\n","        batch_size=self.batch_size)\n","    \n","    return tf_dataset\n","\n","  def __call__(self, train_mode=True):\n","      \"\"\"\n","      Function that makes class instances being callable.\n","      This function calls all methods of the class and\n","      (i)  passes the output of one function as input of the next one\n","      (ii) writes the method outputs as additional class attributes. This \n","      is useful for fetching some of these outputs later.\n","\n","      Inputs\n","      ------\n","      - train_mode (bool): Whether to prepare the data for training or for \n","        inference only, e.g. when you have only a test set to pass. \n","        Default = True\n","\n","      Return\n","      ------\n","      - Outputs and in the same time, attributes of the class \n","        - self.tf_train_dataset\n","        - self.tf_validation_dataset\n","        - self.tf_test_dataset\n","\n","      Note: If train_mode = False, 'tf_train_dataset' and \n","      'tf_validation_dataset' are dummy datasets, containing only the first and \n","      second entry of the test set, respectively. \n","      The test set contains the full input data. This is done, in order to \n","      change the conversion pipeline as little as possible, as it builds complex\n","      data structures i.e. HuggingFace dictionary, tf.Data.dataset.\n","      \"\"\"\n","      self.text_and_labels = self.get_text_and_labels(self.df, \n","                                                      self.anomalies, \n","                                                      self.text_input)\n","      if train_mode == True:\n","        self.df_train, self.df_validation, self.df_test = self.train_validation_test_split(self.text_and_labels, \n","                                                                                          self.first_split_prop, \n","                                                                                          self.second_split_prop,\n","                                                                                          self.random_state)\n","      elif train_mode == False: # inference only\n","        self.df_train, self.df_validation, self.df_test = self.build_dummy_datasets(self.text_and_labels)\n","\n","\n","      self.HF_dataset, self.train_dataset_HF, self.validation_dataset_HF, self.test_dataset_HF = self.convert_to_HF_dataset(self.df_train, \n","                                                                                                                            self.df_validation, \n","                                                                                                                            self.df_test)\n","      self.tokenized_dataset, self.train_set_len = self.tokenize_the_BERT_way(self.HF_dataset, \n","                                                                              self.BERT_model_name, \n","                                                                              self.max_length)\n","      \n","      print(\"Converting tokenized_dataset into tf.data.Dataset datasets...\")\n","      self.tf_train_dataset = self.convert_to_TensorFlow_datasets(tokenized_dataset_to_convert = self.tokenized_dataset, key = \"train\")\n","      self.tf_validation_dataset = self.convert_to_TensorFlow_datasets(tokenized_dataset_to_convert = self.tokenized_dataset, key = \"validation\")\n","      self.tf_test_dataset = self.convert_to_TensorFlow_datasets(tokenized_dataset_to_convert = self.tokenized_dataset, key = \"test\")\n","    \n","      print(\"\\n Structure of the train tf.Data.dataset:\\n\", self.tf_train_dataset)\n","      print(\"\\n\")\n","      print(\"Content of one batch: the narratives have been converted into 'input_ids', 'token_type_ids', 'attention_mask' (see below)\")\n","      print(\"The multilabels are shown last ('array'). \\n\")\n","      print(next(self.tf_train_dataset.as_numpy_iterator()))\n","      print(\"\\n\")\n","      print(\"convert_to_TensorFlow_datasets() done\")\n","      print(30*\"*\", \"\\n\")\n","\n","      return self.tf_train_dataset, self.tf_validation_dataset, self.tf_test_dataset"],"metadata":{"id":"SHHywS4ZBq5y","executionInfo":{"status":"ok","timestamp":1666526189312,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["### GetBERTModel"],"metadata":{"id":"PS-T8pXIIIAp"}},{"cell_type":"code","source":["class GetBERTModel():\n","  '''\n","  Attributes\n","  ----------\n","  BERT_model_name : string, Important: has to be the same model that was passed\n","  to instantiate a DataPrepMultilabelBERT object!\n","  i.e. \"google/bert_uncased_L-12_H-768_A-12\" for BERT BASE, \n","  or one of the Small BERTs: \"google/bert_uncased_L-12_H-128_A-2\" \n","  or #google/bert_uncased_L-2_H-128_A-2\" (BERT tiny).\n","  Therefore, it is best to pass the attribute of the 'DataPrepMultilabelBERT' \n","  object as input. \n","  https://huggingface.co/google/bert_uncased_L-12_H-768_A-12\n","\n","  num_classes: int, in our multilabel case: num_classes = len(anomalies)\n","\n","  trainable_layers: list of int, START at ZERO! [0, ..., 11]\n","    layers of BERT that should be trainable. \n","    'trainable_layers = None' if all layers should be frozen.\n","\n","  Methods\n","  ----------\n","  - __init__(), like every class\n","  - prepare_config()\n","  - download_weights()\n","  - make_layers_trainable()\n","  - __call__() \n","    Reminder: The __call__() method is called by executing 'object()', \n","    where 'object' is an instance of the class.\n","\n","  Attributes created by the class's __call__() method\n","  ------------------------------------------------\n","  Attributes that are created within __call__(). They are the outputs of the \n","  class's own methods.\n","  - config: output of prepare_config()\n","  - downloaded_model: output of download_weights() and then of make_layers_trainable()\n","  '''\n","  def __init__(self, \n","               BERT_model_name, \n","               num_classes,\n","               trainable_layers):\n","\n","    self.BERT_model_name = BERT_model_name\n","    self.num_classes = num_classes\n","    self.trainable_layers = trainable_layers\n","  \n","\n","  def prepare_config(self, BERT_model_name, num_classes):\n","    \"\"\"\n","    Get the BertConfig of the model from Hugging Face and set customize it\n","    if necessary, e.g. in order to get the hidden_states outputs.\n","    \n","    BertConfig is the configuration class to store the configuration \n","    of a BertModel (Pytorch) or a TFBertModel (TensorFlow). \n","    \n","    It is used to instantiate a BERT model according to the specified arguments,\n","    defining the model architecture. \n","\n","    Instantiating a configuration with the defaults will yield a similar \n","    configuration to that of the BERT bert-base-uncased architecture.\n","\n","    Configuration objects inherit from PretrainedConfig and can be used \n","    to control the model outputs. \n","\n","    Read the documentation from PretrainedConfig for more information.\n","    https://huggingface.co/docs/transformers/model_doc/bert\n","\n","    Inputs\n","    -------\n","    - BERT_model_name\n","    - num_classes\n","\n","    Return\n","    -------\n","    - 'config': configuration to pass to the HuggigFace \n","    TFAutoModel.from_pretrained() method that gets the model (weights) from the\n","    HuggingFace library.\n","    \"\"\"\n","    from transformers import AutoConfig\n","    # AutoXXX functions bring more flexibility regarding model checkpoints\n","    # https://github.com/huggingface/transformers/issues/5587\n","\n","    config = AutoConfig.from_pretrained(BERT_model_name)\n","    # Loading a model from its configuration file does not load the model weights. \n","    # It only affects the modelâ€™s configuration. \n","    # We use from_pretrained() below, to load the model weights.\n","\n","    config.output_hidden_states = True # we want access to the hidden states' outputs\n","    config.num_labels = num_classes\n","    \n","    print('Configuration: \\n\\n', config)\n","    print(\"prepare_config() done\")\n","    print(30*\"*\", \"\\n\")\n","    return config\n","\n","  \n","  def download_weights(self, config, BERT_model_name):\n","    '''\n","    Download the model (the weights) and pass our configuration,\n","    as defined in prepare_config().\n","    Inputs\n","    -------\n","    - config: output of the prepare_config() method\n","    - BERT_model_name\n","\n","    Return \n","    ---------\n","    - downloaded_model: downloaded model from HuggingFace ðŸ¤— library, \n","      configured according to 'config'.\n","    '''\n","    from transformers import TFAutoModel\n","    # Download the Transformers BERT model\n","    print(\"Downloading BERT model from Hugging Face...\")\n","    downloaded_model = TFAutoModel.from_pretrained(BERT_model_name, \n","                                                    config = config, \n","                                                    from_pt = True) # From PyTorch\n","    return downloaded_model\n","\n","\n","  def make_layers_trainable(self, downloaded_model, trainable_layers):\n","    '''\n","    Make the desired layers of the model trainable.\n","    Inputs\n","    -----\n","    - downloaded_model: output of download_weights() method\n","    - trainable_layers: list of int, START at ZERO! [0, ..., 11]\n","      Layers of BERT that should be trainable. \n","      'trainable_layers = None' if all layers should be frozen.\n","\n","    Return\n","    -----\n","    - downloaded_model with desired layer training setting\n","    '''\n","    # This gives the whole bert base model and sets all layers' trainable attribute at once\n","    # See this post: https://discuss.huggingface.co/t/fine-tune-bert-models/1554/6\n","    for layer in downloaded_model.layers: # this is the main layer\n","       \n","        if trainable_layers != None:\n","          layer.trainable = True # this makes ALL layers trainable\n","\n","        else:\n","          layer.trainable = False\n","\n","    # Individually make the desired layers (un)trainable\n","    # see https://stackoverflow.com/questions/71336067/how-to-freeze-some-layers-of-bert-in-fine-tuning-in-tf2-keras\n","    nb_layers = len(downloaded_model.bert.encoder.layer)\n","    print(\"\\n Number of layers present:\", nb_layers, \"(12 in the case of BERT BASE).\\n\")\n","\n","    if trainable_layers != None:\n","      all_layers = [0,1,2,3,4,5,6,7,8,9,10,11]\n","      frozen_layers  = [i for i in all_layers if i not in trainable_layers] \n","\n","      for layer_ID in frozen_layers:\n","        downloaded_model.bert.encoder.layer[layer_ID].trainable = False # freeze layer\n","\n","    # Show  the status of all layers\n","    for i in range(nb_layers):\n","      print(f\"Layer {i} trainable:\", downloaded_model.bert.encoder.layer[i].trainable)\n","\n","    print(\"\\n\")\n","    print(\"make_layers_trainable() done\")\n","    print(30*\"*\", \"\\n\")\n","    \n","    # return the updated model\n","    return downloaded_model\n","\n","  def __call__(self):\n","    self.config = self.prepare_config(self.BERT_model_name, self.num_classes)\n","    self.downloaded_model = self.download_weights(self.config, self.BERT_model_name)\n","    self.downloaded_model = self.make_layers_trainable(self.downloaded_model, self.trainable_layers)\n","\n","    return self.downloaded_model"],"metadata":{"id":"eJKkTy9vMs3-","executionInfo":{"status":"ok","timestamp":1666526189313,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Build the model"],"metadata":{"id":"BM5WrV49PnhG"}},{"cell_type":"markdown","source":["#### ConcatSlice"],"metadata":{"id":"E_hs2BiaO4oq"}},{"cell_type":"code","source":["class ConcatSlice(tf.keras.layers.Layer):\n","    '''\n","    Custom, nontrainable model layer, inheriting from tf.keras.Layer. \n","    Defines which model outputs to keep, e.g. last_hidden_state, hidden_layer[i] etc.\n","    and whether to concatenate or flatten them.\n","\n","    Attributes \n","    --------\n","    - layer_to_get : string 'last_hidden_state' or integer -2, -3, etc. or 'concat'\n","      Which layers' output(s) to use. \n","      '-2' means the second-to-last hidden state, 'concat' will produce a richer \n","      embedding for each word in the sequence. Default = 'last_hidden_state'\n","    \n","    - emb_to_use = (string) 'CLS' or 'flatten', use the embedding of the CLS token\n","      (start of the sequence) or flatten the output, default = 'CLS'\n","\n","    - layers_to_concat : list of int, from 1 to 12. Which of BERT's layers \n","      # to use for concatenation. Applies only when layer_to_get = 'concat', \n","      default = [9, 10, 11, 12]\n","\n","    Methods\n","    --------\n","    - __init__()\n","    - get_config()\n","    - use_CLS_or_flatten()\n","    - concat_BERT_layers()\n","    - __call__()\n","\n","    '''    \n","    def __init__(self, \n","                 layer_to_get = 'last_hidden_state',\n","                 emb_to_use = 'CLS',\n","                 layers_to_concat = [9, 10, 11, 12], \n","                 **kwargs):\n","      \n","      # Initialize the parent class, i.e. tf.keras.layers.Layer\n","      # OLD CODE\n","      #super(ConcatSlice, self).__init__()\n","      super(ConcatSlice, self).__init__(**kwargs)\n","      \n","      # Class attributes\n","      self.layer_to_get = layer_to_get\n","      self.emb_to_use = emb_to_use\n","      self.layers_to_concat = layers_to_concat\n","\n","      # OLD CODE\n","      #super(ConcatSlice, self).__init__(**kwargs)\n","\n","      # Print information to the user\n","      if self.layer_to_get == 'last_hidden_state':\n","        self.layers_to_concat = None # important for the creation of the classif\n","        # report in pd.DataFrame format\n","        print(\"Will get the output(s) of layer(s):\", self.layer_to_get)\n","\n","      elif type(self.layer_to_get) == int:\n","        # In this case, self.layer_to_get has to be -2 or -3 etc.\n","        self.layers_to_concat = None # important for the creation of the classif\n","        # report in pd.DataFrame format\n","        print(f\"Will get the output(s) of layer(s): [hidden_state][{self.layer_to_get}]\")\n","      \n","      elif self.layer_to_get == 'concat': \n","        print(\"Will get the output(s) of layer(s):\", self.layer_to_get)\n","        print(f\"Will concatenate the outputs of BERT layers\", self.layers_to_concat, \".\\n\")\n","      \n","      print(\"Setting used (CLS or flatten?):\", emb_to_use, \"\\n\")\n","\n","    def get_config(self):\n","      \"\"\"\n","      In order to save/load a model with custom-defined layers, \n","      or a subclassed model, you should overwrite the get_config \n","      and optionally from_config methods of the parent class.\n","      Returns the config of the layer.\n","\n","      A layer config is a Python dictionary (serializable, \n","      i.e. can be written into .json or .pkl) containing the configuration \n","      of a layer. \n","      The same layer can be reinstantiated later (without its trained weights) \n","      from this configuration.\n","      see https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects\n","      \"\"\"\n","      config = super(ConcatSlice, self).get_config().copy()\n","      config.update({\"layer_to_get\": self.layer_to_get,\n","                     \"emb_to_use\": self.emb_to_use,\n","                     \"layers_to_concat\": self.layers_to_concat})\n","      return config\n","    \n","    def use_CLS_or_flatten(self, x, setting):\n","      \"\"\"\n","      Slice or Flatten() the input 'x', depending on the chosen 'setting'\n","      \"\"\"\n","      #print(\"setting used:\", setting, \"\\n\")\n","      if setting == 'CLS':\n","        # keep the embedding of the CLS token only\n","        x = x[:, 0, :]\n","\n","      elif setting == 'flatten':\n","        # flatten the output using keras layer Flatten()\n","        x = Flatten()(x)\n","        # expected dimension: embedding size * max_length,\n","        # e.g. 768 * 200 = 153,600 if using bert_base\n","\n","      return x\n","\n","    def concat_BERT_layers(self, inputs, layers_to_concat):\n","      \"\"\"\n","      Concatenante the 'layers_to_concat' layers of BERT\n","      \"\"\"\n","      list_for_concat = []\n","      for i in layers_to_concat:\n","        list_for_concat.append(inputs['hidden_states'][i])\n","      \n","      return tf.keras.layers.Concatenate(axis = -1)(list_for_concat)\n","\n","    def call(self, inputs):\n","      if self.layer_to_get == 'last_hidden_state':\n","        x = inputs['last_hidden_state']\n","        x = self.use_CLS_or_flatten(x, setting = self.emb_to_use)\n","      \n","      elif type(self.layer_to_get) == int:\n","        # In this case, self.layer_to_get has to be -2 or -3 etc.\n","        x = inputs['hidden_states'][self.layer_to_get]\n","        x = self.use_CLS_or_flatten(x, setting = self.emb_to_use)\n","\n","      elif self.layer_to_get == 'concat': \n","        x = self.concat_BERT_layers(inputs, self.layers_to_concat)\n","        # expected dimension: embedding size * layers_to_concat,\n","        # e.g. 768 * 4 = 3072 if using bert_base and concatenating the last 4 layers\n","        x = self.use_CLS_or_flatten(x, setting = self.emb_to_use)\n","\n","      #print(\"Shape of the input to the first dense layer:\\n\", x.shape, \"\\n\")\n","\n","      return x"],"metadata":{"id":"dyfeB2DKfgvR","executionInfo":{"status":"ok","timestamp":1666526189313,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["#### ClassifTransformerModelML"],"metadata":{"id":"YkdTylrZO8Xn"}},{"cell_type":"code","source":["class ClassifTransformerModelML(tf.keras.Model):\n","    '''\n","    Multilabel, multiclass transformer(BERT)-based model for text classification\n","\n","    Attributes \n","    --------\n","    - num_classes : (int) number of classes to classify to.\n","    - max_length : (int) sequence length, e.g. 200. Better pass the \n","      .max_length attribute of the Class 'DataPrepMultilabelBERT' object created previously\n","    - transformer_model : transformer model returned when calling a GetBERTModel instance\n","    \n","    Additional attributes created upon instantiation\n","    -------------------------------------------------\n","    - input_ids\n","    - concat_slice\n","    - dense1\n","    - dense2\n","    - transformer_model._saved_model_inputs_spec\n","\n","    Methods\n","    -------\n","    - __init__()\n","    - get_config()\n","    - call()\n","    - summary()\n","\n","    Notes\n","    --------\n","    We are 'subclassing' the 'tf.keras.Model' class (see documentation of tf.keras.Model), \n","    i.e. we define our layers in __init__() and implement the model's forward pass in call().\n","    '''\n","    def __init__(self, num_classes, max_length, transformer_model, **kwargs):\n","\n","        tf.keras.backend.clear_session() # ensure that no model is present in the memory\n","\n","        # Initialize the parent class, i.e. tf.keras.Model\n","        # OLD code\n","        #super(ClassifTransformerModelML, self).__init__()\n","        super(ClassifTransformerModelML, self).__init__(**kwargs)\n","\n","        ## Attributes specific to our model\n","        self.max_length = max_length\n","        self.input_ids = Input(shape=(self.max_length,), name='input_ids', dtype='int32')\n","        self.num_classes = num_classes\n","        # Define model layers\n","        self.transformer_model = transformer_model\n","        # Custom layer\n","        self.concat_slice = ConcatSlice() \n","        # () will call the default values, i.e. returns the 'CLS' embedding\n","        # of 'last_hidden_state'.\n","        \n","        # Classification head \n","        self.dense1 = Dense(units = 32, activation='relu', name='dense1')\n","        self.dense2 = Dense(units = self.num_classes, activation='sigmoid', name='dense2')\n","\n","        #########################\n","        # RESET THE INPUT SPECS\n","        #########################\n","        # Keras saves the input specs on the first call of the model. \n","        # When loading a pretrained model with transformers using the \n","        # 'from_pretrained' class method of TFPretrainedModel, the networks \n","        # is first fed dummy inputs. So the saved models expect their\n","        # input tensors to be of sequence length 5 (that is the length of the \n","        # dummy inputs). \n","        # To change that behaviour, reset the input specs before saving to \n","        # a saved model like this.\n","        # Ioannis thinks its crucial to reset these specs before the first call \n","        # of the pretrained model, that is why this is done inside __init__().\n","        # see https://github.com/keras-team/keras/issues/14345#issuecomment-1118569356\n","\n","        # Create dummy tensors with the shape of our actual features\n","        dummy_array = list(range(1, self.max_length + 1))\n","        dummy_tensor = tf.constant([dummy_array], dtype = tf.int64) # it has to be int64\n","        # Dummy model input\n","        features = {\"input_ids\": dummy_tensor, \n","                    \"attention_mask\": dummy_tensor, \n","                    \"token_type_ids\": dummy_tensor}\n","        # Set the save spec using the dummy input and write them as class attributes           \n","        self.transformer_model._saved_model_inputs_spec = None\n","        self.transformer_model._set_save_spec(features)\n","        #print(\"saved_model_inputs_spec have been overwritten to:\")\n","        #print(self.transformer_model._saved_model_inputs_spec)\n","        print(\"\\n\")\n","        ###########################\n","\n","    def get_config(self):\n","        \"\"\"\n","        In order to save/load a model with custom-defined layers, \n","        or a subclassed model, you should overwrite the get_config \n","        and optionally from_config methods.\n","        Returns the config of the layer.\n","\n","        A layer config is a Python dictionary (serializable, \n","        i.e. can be written into .json or .pkl) containing the configuration \n","        of a layer. \n","        The same layer can be reinstantiated later (without its trained weights) \n","        from this configuration.\n","        see https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects\n","        \"\"\"\n","        config = super(ClassifTransformerModelML, self).get_config().copy()\n","        config.update({\"num_classes\": self.num_classes, \n","                        \"max_length\": self.max_length, \n","                        \"transformer_model\": self.transformer_model#,\n","                        # \"infer\": self.infer,\n","                        # \"save_transformer\": self.save_transformer\n","                       })\n","        # print(\"CONFIG:\")\n","        # print(config, \"\\n\")\n","        return config\n","    \n","    def call(self, inputs):\n","        #print(\"The call() method of a ClassifTransformerModelML class object has been called.\\n\")\n","        # Connect the layers the functional way\n","        x = self.transformer_model(inputs)\n","        x = self.concat_slice(x) \n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        #print(\"Shape of final dense layer's output:\\n\", x.shape, \"\\n\")\n","\n","        return x\n","\n","    def summary(self, print_fn=None):\n","        \"\"\"\n","        Print the model's summary\n","        \"\"\"\n","        # Instantiate an object of the class tf.keras.Model\n","        model = Model(inputs=[self.input_ids], \n","                      outputs=self.call(self.input_ids),\n","                      name='Generic model instance')\n","        return model.summary(print_fn = print_fn)"],"metadata":{"id":"I0ZcNL2BmTbe","executionInfo":{"status":"ok","timestamp":1666526189756,"user_tz":-120,"elapsed":18,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["### Compile"],"metadata":{"id":"ccEpbl8ySr3d"}},{"cell_type":"code","source":["def compile_transformer(model, \n","                        batch_size,\n","                        train_set_len,\n","                        num_epochs = 10,\n","                        metric = 'binary_accuracy',\n","                        optimizer_type = 'AdamW'):\n","  \"\"\"\n","  Inputs\n","  -------\n","  - model: instance of 'ClassifTransformerModelML' class\n","  - batch_size\n","  - train_set_len (int): length of the train set; see the outputs of the \n","    __call__() method of DataPrepMultilabelBERT\n","  - num_epochs (int): number of epochs, default = 10\n","  - metric: default = 'binary_accuracy' (calculates how often predictions \n","    match binary labels. This is the default for a multilabel classification)\n","  - optimizer_type: pass 'AdamW' or 'Adam', default = 'AdamW'\n","\n","  Return\n","  -------\n","  - model: compiled model\n","  - optimizer\n","  - num_epochs: for further use by .train()\n","  - loss\n","  - metric\n","  \"\"\"\n","  ########################  \n","  # Create optimizer\n","  ########################\n","  if optimizer_type == 'Adam':\n","    optimizer = keras.optimizers.Adam(model,\n","                                      learning_rate=5e-05,\n","                                      epsilon=1e-08,\n","                                      decay=0.01,\n","                                      clipnorm=1.0)\n","\n","  elif optimizer_type == 'AdamW': \n","    # The create_optimizer function in the Transformers library creates an AdamW \n","    # optimizer with weight and learning rate decay. This performs very well for \n","    # training most transformer networks - we recommend using it as your default \n","    # unless you have a good reason not to! Note, however, that because it decays \n","    # the learning rate over the course of training, it needs to know how \n","    # many batches it will see during training.\n","    # See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.create_optimizer\n","    from transformers import create_optimizer\n","    batches_per_epoch = train_set_len // batch_size\n","    total_train_steps = int(batches_per_epoch * num_epochs)\n","\n","    optimizer, schedule = create_optimizer(init_lr = 2e-5,\n","                                          # The initial learning rate for the schedule after the warmup \n","                                          # (so this will be the learning rate at the end of the warmup)\n","                                          num_warmup_steps=0,\n","                                          num_train_steps=total_train_steps)\n","\n","  ########################  \n","  # Set loss and metrics\n","  ########################\n","  loss = tf.keras.losses.BinaryCrossentropy(from_logits = False) \n","  # value in [-inf, inf] when from_logits=True or a probability (i.e, value in [0., 1.] when from_logits = False).\n","  metric = tf.keras.metrics.BinaryAccuracy(metric) \n","\n","  ########################  \n","  # Compile the model\n","  ########################\n","  # Defining a variable, e.g. 'compiled_model' and assigning it the output of\n","  # model.compile returns a NoneType object, because the compile() method does \n","  # not return a model, but only updates its values (?)\n","  # Hence, we need to return model, which is not any more the same as the input\n","  # that was received.\n","  compiled_model = model.compile(optimizer = optimizer,\n","                                loss = loss, \n","                                metrics = metric)\n","  \n","  return model, optimizer, num_epochs, loss, metric"],"metadata":{"id":"CHuJqNDTCO--","executionInfo":{"status":"ok","timestamp":1666526189756,"user_tz":-120,"elapsed":15,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Define CALLBACKS\n","def get_callbacks(include_tensorboard_CB = False, **kwargs):\n","  \"\"\"\n","  Define callbacks for model training:\n","  - tf.keras.callbacks.TerminateOnNaN \n","  - tf.keras.callbacks.EarlyStopping \n","\n","  Inputs\n","  -------\n","  - include_tensorboard_CB (bool), whether to include the tensorboard_callback, \n","    default = False\n","\n","  *Optional keyword arguments:\n","  - experiment_dir: (str) directory; where to save the training logs used by TensorBoard.\n","    e.g. /content/drive/MyDrive/data/saved models/Yannis/BERT/h5_tests/test1\n","    No slash at the end.\n","\n","  Return\n","  -------\n","  - list of callbacks\n","  \"\"\"\n","  TON = callbacks.TerminateOnNaN() # Callback that terminates training when a NaN loss is encountered\n","\n","  early_stopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', # 'binary_accuracy' calculates how often predictions match binary labels.\n","                                          min_delta=0.003, # original: 0.005\n","                                          patience = 3, \n","                                          mode = 'max', \n","                                          restore_best_weights = True, \n","                                          verbose = 1)\n","  if include_tensorboard_CB == True:\n","    # Load the TensorBoard notebook extension\n","    import datetime\n","    %reload_ext tensorboard\n","\n","    # Clear any logs from previous runs\n","    %rm -rf ./logs/\n","\n","    # 'Unpack' the optional keyword arguments\n","    experiment_dir = kwargs['log_dir_name']\n","\n","    # Define full path for writing logs\n","    log_dir = experiment_dir + \"/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    print(\"Writing logs in:\", experiment_dir + \"/logs/fit/\")\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n","                                                          histogram_freq=1)\n","    \n","    \n","    return [TON, early_stopping, tensorboard_callback]\n","  \n","  else:\n","    return [TON, early_stopping]"],"metadata":{"id":"6wU-EG-CZaPK","executionInfo":{"status":"ok","timestamp":1666526189756,"user_tz":-120,"elapsed":13,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### Fit"],"metadata":{"id":"0ePuSA9ZUuil"}},{"cell_type":"code","source":["def train_transformer(model, train_dataset, validation_data, my_callbacks, num_epochs):\n","  \"\"\"\n","  Input\n","  ------\n","  - 'model': compiled transformer model\n","  - 'my_callbacks': list of callbacks, see output of get_callbacks(). Do not \n","    mix up with 'callbacks' which is the name of the loaded Keras package.\n","  - 'num_epochs' (int): number of epochs to train\n","\n","  Return\n","  -------\n","  - trained_model with updated weights\n","  - training_history: for plotting the training process\n","  - exec_time: execution time in minutes\n","\n","  Note\n","  -----\n","  The call() method of a ClassifTransformerModelML class object is called twice,\n","  once on the training, once on the validation data.\n","  \"\"\"\n","  # Time the function execution\n","  start_time = time.time()\n","  print(7*'-', f\"Execution started...\", 7*'-')\n","\n","  training_history = model.fit(x = train_dataset,\n","                              validation_data = validation_data,\n","                              epochs = num_epochs, \n","                              callbacks = my_callbacks,\n","                              verbose = 1) # use verbose 1 to show the progress bar\n","\n","  # Calculate and print time duration\n","  print(7*'-', f\"Training finished!\", 7*'-')\n","  end_time = time.time()\n","  exec_time = np.round((end_time - start_time)/60,1)\n","  print(f\"--- It took {exec_time} minutes --- \\n\\n\")\n","\n","  # Plot the train history\n","  plot_train_history(training_history, 'binary_accuracy', '')\n","  plot_train_history(training_history, 'loss', '')\n","\n","  # Return the 'model', which has been trained by now.\n","  # Since the fit() method does not return a model instance, we cannot\n","  # do this: trained_model = model.fit(...)\n","  return model, training_history, exec_time"],"metadata":{"id":"EhCURsne6XDs","executionInfo":{"status":"ok","timestamp":1666526189757,"user_tz":-120,"elapsed":14,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["### Infer"],"metadata":{"id":"N0rqdRccZjNC"}},{"cell_type":"code","source":["def infer(model, test_dataset, tokenized_dataset, batch_size, threshold, anomalies):\n","    \"\"\"\n","    - Calculate scores on the metrics defined during model.compile()\n","    - Get multilabel predictions y_pred_proba, i.e. 'num_classes' \n","      probabilities for each data entry\n","    - Convert probabilities y_pred_proba into y_pred, i.e. binary (0,1) \n","      multilabels\n","    - Construct the classification report\n","\n","    Inputs\n","    -------\n","    - test_dataset (tf.data.Dataset): best is to use the attribute of a \n","      'DataPrepMultilabelBERT' class instance\n","    - tokenized_dataset (HuggingFace dataset). Best is to use the attribute \n","      of a 'DataPrepMultilabelBERT' class instance\n","    - batch_size (int): best is to use the attribute of a \n","      'DataPrepMultilabelBERT' class instance\n","    - threshold for the probability --> binary conversion\n","    - anomalies (list of str): Used to label the classification report.\n","      Best use the .anomalies attribute of a 'DataPrepMultilabelBERT' class \n","      object\n","\n","    Return\n","    -------\n","    - evaluation_scores: output of tensforflow method 'model.evaluate()'\n","    - y_pred_proba: output of tensforflow method 'model.predict()'. \n","      Multilabel probabilities.\n","    - y_pred: binary (0,1) multilabels\n","    - y_test: binary (0,1) multilabels\n","    - clf_rep: classification report in dictionary format\n","\n","    Notes\n","    ------\n","    Ioannis initially created this function as a 'ClassifTransformerModelML'\n","    class method. Because the latter is a custom model, the method was\n","    'untraced' and therefore was not part of *reloaded* models. \n","    This is why the infer() function is now defined outside the class. \n","    \"\"\"\n","    print(\"Evaluation scores on the test set (usually: loss and accuracy):\")\n","    evaluation_scores = model.evaluate(test_dataset)\n","    print(evaluation_scores, '\\n')\n","    \n","    print(\"Predicting multilabel probabilities y_pred_proba...\")\n","    y_pred_proba = model.predict(test_dataset, #tokenized_dataset[\"test\"]['input_ids'], \n","                                batch_size = batch_size, \n","                                verbose = 1)\n","    print(\"Shape of y_pred_proba:\", y_pred_proba.shape, '\\n')\n","    print(\"Example of entry in y_pred_proba:\", y_pred_proba[0], '\\n')\n","\n","    print(\"Converting probabilities into binary (0,1) multilabel 'y_pred' using threshold =\", threshold)\n","    y_pred = y_prob_to_y_pred_ML(y_pred_proba, threshold = threshold)\n","    print(\"Example of entry in y_pred:\", y_pred[0])\n","\n","    print(\"\\n Getting y_test from tokenized test dataset in HuggingFace dataset format to build the classification report.\\n\")\n","    y_test = tokenized_dataset[\"test\"]['labels']\n","\n","    # Classification report\n","    clf_rep = classification_report(y_test, y_pred, output_dict = True)\n","    print(f\"\\n\\n Classification Report: \\n {classification_report(y_test, y_pred, target_names = anomalies)}\\n\")\n","\n","    return evaluation_scores, y_pred_proba, y_pred, y_test, clf_rep"],"metadata":{"id":"PN1A97GeeKN5","executionInfo":{"status":"ok","timestamp":1666526189757,"user_tz":-120,"elapsed":13,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["### Save"],"metadata":{"id":"IgkHuRvQs0Of"}},{"cell_type":"code","source":["def save_transformer(model, experiment_dir):\n","  \"\"\"\n","  Saves the model using model.save(), an alias of tf.keras.models.save_model()\n","  method\n","\n","  Inputs\n","  ------\n","  - experiment_dir (str): directory of the experiment. The data will be saved here.\n","\n","  Return\n","  ------\n","  - None; this function saves data externally\n","\n","  Notes on execution\n","  -------------------\n","  - The experiment_dir is created automatically, if not already existing.\n","  - If you are about to overwrite a saved model, you will get a prompt.\n","    You could change this setting to overwritting by default, e.g. if you let\n","    a model train for hours and want to make sure it is saved (in case you are \n","    not there to type the 'y' inside the promt). \n","  - It is normal to see 7x the msg.\n","    'The call() method of a ClassifTransformerModelML class object \n","    has been called.'; It remains unclear why the object is called 7x during save.\n","  - The file takes some time to appear in the experiment_dir \n","    (at least if it's a Google Drive folder).\n","  - Executing for a model containing BERT, will probably print something like \n","    'WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, etc.'\n","    Apparently there are ~80 functions that 'will not be *directly* callable after loading.'\n","    Ioannis thinks that this is not a problem, since the loaded model can be \n","    successfully compiled and trained.\n","\n","  Notes on available formats for saving models\n","  ---------------------------------------------\n","  There are two formats you can use to save an entire model to disk: \n","  - the TensorFlow SavedModel format\n","  - the older Keras H5 format. \n","  \n","  **The recommended format is TensorFlow SavedModel.**\n","\n","  SavedModel is the more comprehensive save format that saves \n","  - the model architecture, \n","  - weights, \n","  - the traced Tensorflow subgraphs of the call functions. \n","  \n","  This enables Keras to restore both built-in layers as well as custom objects.\n","  When saving in TensorFlow SavedModel format, a folder is created containing \n","  the files/folders:\n","  - assets  \n","  - keras_metadata.pb  \n","  - saved_model.pb  \n","  - variables\n","\n","  On the other hand, HDF5 is a single file containing \n","  - the model's architecture, \n","  - weights values (which were learned during training), \n","  - compile() information (if compile() was called)\n","  - the optimizer and its state, if any (this enables you to restart training \n","  where you left)\n","  See https://www.tensorflow.org/guide/keras/save_and_serialize\n","  \n","  HDF5 is a light-weight alternative to TensorFlow SavedModel with imitations\n","  (see https://www.tensorflow.org/guide/keras/save_and_serialize#limitations)\n","\n","  /!\\ In any case: DO NOT USE .pkl (pickle) file format for DEEP LEARNING models !\n","\n","  Notes on implementation\n","  ------\n","  Ioannis initially created this function as a 'ClassifTransformerModelML'\n","  class method. Because the latter is a custom model, the method was\n","  'untraced' and therefore was not part of *reloaded* models. \n","  This is why the infer() function is now defined outside the class. \n","  \"\"\"\n","  # Define options for saving to SavedModel\n","  # We save as much info as possible\n","  tf_save_model_opts = tf.saved_model.SaveOptions(namespace_whitelist=None,\n","                                                  save_debug_info = True, # default is False\n","                                                  function_aliases=None,\n","                                                  experimental_io_device=None,\n","                                                  experimental_variable_policy=None,\n","                                                  experimental_custom_gradients=True)\n","\n","  # model.save() is an alias for tf.keras.models.save_model()\n","  # Difference between tf.saved_model.save and tf.keras.model.save: none essentially\n","  model.save(\n","      filepath = experiment_dir,\n","      overwrite=True, #False: ask the user with a manual prompt\n","      include_optimizer=True, # save optimizer's state together\n","      save_format=None, # Either 'tf' for Tensorflow SavedModel or 'h5' HDF5, defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X.\n","      signatures=None, # Signatures to save with the SavedModel. Applicable to the 'tf' format only\n","      options = tf_save_model_opts,\n","      save_traces=True # when save_traces=False, all custom objects must have defined get_config/from_config methods. When loading, the custom objects must be passed to the custom_objects argument. save_traces=False reduces the disk space used by the SavedModel and saving time.\n","  )\n","  print(\"Model successfully saved in:\\n\", experiment_dir)"],"metadata":{"id":"qAaAB8wmc9Cu","executionInfo":{"status":"ok","timestamp":1666526189758,"user_tz":-120,"elapsed":14,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["### Load the model"],"metadata":{"id":"RStFFEzEARft"}},{"cell_type":"code","source":["def load_saved_transformer(filepath, \n","                           compile=True):\n","  \"\"\"\n","  Inputs\n","  -------\n","  - filepath (str): directory where to load the model from\n","  - compile (bool): whether to compile the model after loading, default = True.\n","    Works only if the original model was compiled, and saved along with \n","    the optimizer. \n","\n","  Return\n","  -------\n","  - loaded transformer model\n","\n","  Notes\n","  ------\n","  Use the 'custom_object' parameter of keras.models.load_model() while loading \n","  the model if you've custom layer-like stuff.\n","  Alternatively, the @tf.keras.utils.register_keras_serializable() decorator can\n","  be used in the CustomLayer or CumstomModel class definitions,\n","  see https://stackoverflow.com/questions/62280161/saving-keras-models-with-custom-layers\n","  \"\"\"\n","  print(f\"Loading the model from directory \\n {filepath}\\n\")\n","  from tensorflow.keras.models import load_model\n","  loaded_model = keras.models.load_model(filepath = filepath, \n","                                        custom_objects={\"ClassifTransformerModelML\": ClassifTransformerModelML, \n","                                                        \"ConcatSlice\": ConcatSlice},\n","                                        compile = compile \n","                                        )\n","  print(\"Model successfully loaded.\")\n","  return loaded_model"],"metadata":{"id":"Ltm2ovsJP1a-","executionInfo":{"status":"ok","timestamp":1666526189758,"user_tz":-120,"elapsed":13,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# GLOBAL FUNCTION\n"],"metadata":{"id":"LS3xxX8UagNr"}},{"cell_type":"code","source":["def train_load_transformer_model(dir_name, experiment_name, \n","                                 df,\n","                                 num_epochs, \n","                                 anomalies = Anomaly_RootLabels_columns,\n","                                 threshold = 0.5, \n","                                 train_mode=False, \n","                                 load_model=False, \n","                                 save_and_overwrite_model=False):\n","  \"\"\"\n","  Inputs\n","  --------\n","  - dir_name (str): directory where to save the model, training logs, y_pred, \n","  classification report etc.\n","  - experiment_name (str): experiment name that will be used to name the .pkl files.\n","    A folder entitled [experiment_name] is created within the directory [dir_name] \n","    and contains the files that constitute the TensorFlow SavedModel format. \n","    In other words, this folder contains the model with all its assets.\n","  - df (pd.DataFrame): input data containing at least a column 'Narrative' with \n","    the texts and columns of Anomalies in one-hot format\n","  - num_epochs (int): number of epochs to train\n","  - anomalies (list of str), anomaly root labels. Default = Anomaly_RootLabels_columns \n","  - threshold (float) threshold for probability to boolean conversion \n","    during prediciton, default = 0.5\n","  - train_mode (boool), whether to train a model. Inference only if set to False.\n","    Default = False\n","  - load_model (bool), whether to load an existing model. Default = False\n","  - save_and_overwrite_model (bool), whether to save the trained model. \n","\n","    /!\\ this will overwrite an existing model located in the same directory!\n","    Default = False)\n","\n","  If both train_mode = True and load_model = True, it will train a model, \n","  load it from the file and retrain it.\n","\n","  If train_mode = False and load_model = True, y_pred, clf_rep etc. will be saved\n","  with 'from_reloaded' included in their filename, so that they can be distinguished\n","  from the original files, generated during the initial training of the model.\n","\n","  Return\n","  ------- \n","  None; this function saves data externally \n","  \"\"\"\n","  experiment_dir = dir_name + experiment_name\n","\n","  if train_mode == True:\n","\n","    ############################\n","    # Preprocess for training\n","    preprocess = DataPrepMultilabelBERT(BERT_model_name = \"google/bert_uncased_L-12_H-768_A-12\", \n","                                        df = df, \n","                                        anomalies = anomalies)\n","\n","    tf_train_dataset, tf_validation_dataset, tf_test_dataset = preprocess(train_mode = train_mode)\n","\n","    #############################\n","    # Donwload the BERT model\n","    get_bert_model = GetBERTModel(BERT_model_name = preprocess.BERT_model_name,\n","                                num_classes = len(preprocess.anomalies),\n","                                trainable_layers = [8,9,10,11] # None\n","                                )\n","    bert_model = get_bert_model()\n","\n","    # Build our classification model, including the BERT layer\n","    transformer_model = ClassifTransformerModelML(num_classes = get_bert_model.num_classes,\n","                                                  max_length = preprocess.max_length,\n","                                                  transformer_model = bert_model)\n","    # get_bert_model.num_classes: attribute of the Class 'GetBertModel' object created above\n","    transformer_model.summary()\n","    # if __name__ == '__main__':\n","    #   transformer_model.summary()\n","    # If the source file is executed as the main program, \n","    # the interpreter sets the __name__ variable to have a value â€œ__main__â€. \n","    # If this file is being imported from another module, \n","    # __name__ will be set to the moduleâ€™s name.\n","    # https://www.geeksforgeeks.org/__name__-a-special-variable-in-python/#:~:text=__name__%20is%20one,set%20to%20the%20module's%20name.\n","\n","    ########################\n","    # Compile the model\n","    compiled_transformer_model, optimizer, num_epochs, loss, metric = compile_transformer(transformer_model,\n","                                                    batch_size = preprocess.batch_size, # use the attribute of the object created above\n","                                                    train_set_len = preprocess.train_set_len,\n","                                                    num_epochs = num_epochs, \n","                                                    optimizer_type = 'AdamW')\n","\n","    # If you want to use TensorBoard, define directory where to save the training logs to\n","    my_callbacks = get_callbacks(include_tensorboard_CB = True, \n","                                log_dir_name = experiment_dir)\n","    #######################\n","    # Train the model\n","    trained_transformer_model, training_history, exec_time = train_transformer(compiled_transformer_model, \n","                                                                            preprocess.tf_train_dataset,\n","                                                                            preprocess.tf_validation_dataset,\n","                                                                            my_callbacks, \n","                                                                            num_epochs = num_epochs)\n","\n","\n","    ######################\n","    # Launch TensorBoard\n","    ######################\n","    # Change the pwd, so that TensorBoard may locate the logs\n","    %cd $experiment_dir\n","\n","    #Start TensorBoard through the command line or within a notebook experience. \n","    # The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. \n","    # On the command line, run the same command without \"%\"\n","    %tensorboard --logdir logs/fit\n","    # Takes some time to launch\n","\n","    # For Comparing different executions of your model see \n","    # https://github.com/tensorflow/tensorboard/blob/master/README.md#runs-comparing-different-executions-of-your-model\n","\n","    # # Kill tensorboard (use the appropriate process id)\n","    # !kill 2022\n","\n","    evaluation_scores, y_pred_proba, y_pred, y_test, clf_rep = infer(trained_transformer_model,\n","                                                                  preprocess.tf_test_dataset,\n","                                                                  preprocess.tokenized_dataset,\n","                                                                  preprocess.batch_size,\n","                                                                  threshold = threshold, \n","                                                                  anomalies = preprocess.anomalies)\n","\n","    # Convert the classification report in form of a pd.DataFrame, adding \n","    # additional info\n","    clf_rep_df = convert_clf_rep_to_df_multilabel_BERT_kw_args(clf_rep, \n","                                                        preprocess.anomalies, \n","                                                        preprocessing = 'original',\n","                                                        classifier = 'BERT_BASE',\n","                                                        undersampling = 0,\n","                                                        UNfrozen_layers = '9,10,11,12',\n","                                                        concat_layers = 'None',\n","                                                        comments = 'last_hidden_state_CLS',\n","                                                        experiment_ID = '11_3_3'\n","                                                        )\n","    print(\"\\n Showing clf_rep_df.head():\\n\")\n","    print(clf_rep_df.head())\n","\n","    if save_and_overwrite_model == True:\n","      #################\n","      # SAVE THE MODEL\n","      save_transformer(trained_transformer_model, experiment_dir)\n","\n","      ###############################\n","      # SAVE THE MULTILABEL OUTPUTS\n","      save_ML_outputs(dir_name, experiment_name, \n","                  y_pred_proba = y_pred_proba, \n","                  y_pred = y_pred, \n","                  y_test = y_test, \n","                  clf_rep = clf_rep,\n","                  clf_rep_df = clf_rep_df)\n","\n","      ######################################\n","      # WRITE EXPERIMENT INFO TO .txt file\n","      model_attr_to_save = ['BERT_model_name', 'trainable_layers', \n","                            'num_classes', 'anomalies', 'batch_size',\n","                            'max_length', 'emb_to_use', 'layer_to_get', \n","                            'layers_to_concat', 'use_CLS_or_flatten']\n","      class_objects = [preprocess, \n","                       get_bert_model, \n","                       transformer_model, \n","                       transformer_model.concat_slice]\n","\n","      save_exp_info_to_txt(dir_name = dir_name, \n","                      experiment_name = experiment_name, \n","                      model_attr_to_save = model_attr_to_save, \n","                      model = transformer_model,\n","                      class_objects = class_objects,\n","                      num_epochs = num_epochs, \n","                      loss = loss, \n","                      optimizer = optimizer, \n","                      metric = metric,\n","                      callbacks = my_callbacks,\n","                      threshold = threshold,\n","                      execution_time = exec_time)\n","\n","\n","  if train_mode == True & load_model == True:\n","    # Delete the freshly trained model from the memory, to see if it is really freshly loaded\n","    del trained_transformer_model\n","    # Load a saved model\n","    loaded_model = load_saved_transformer(filepath = experiment_dir)\n","    # Fit the loaded model\n","    loaded_model_retrained, training_history, exec_time = train_transformer(loaded_model,\n","                                                                            preprocess.tf_train_dataset,\n","                                                                            preprocess.tf_validation_dataset,\n","                                                                            my_callbacks)\n","    # Infer using the retrained loaded model\n","    evaluation_scores, y_pred_proba, y_pred, y_test, clf_rep = infer(loaded_model_retrained,\n","                                                                    preprocess.tf_test_dataset,\n","                                                                    preprocess.tokenized_dataset,\n","                                                                    preprocess.batch_size,\n","                                                                    threshold = 0.5, \n","                                                                    anomalies = preprocess.anomalies)\n","\n","  elif train_mode == False:\n","    ##################################\n","    # Preprocess for inference only\n","    preprocess = DataPrepMultilabelBERT(BERT_model_name = \"google/bert_uncased_L-12_H-768_A-12\", \n","                                        df = df, \n","                                        anomalies = Anomaly_RootLabels_columns)\n","\n","    tf_train_dataset, tf_validation_dataset, tf_test_dataset = preprocess(train_mode = train_mode)\n","\n","    if load_model == True:\n","      # Load a saved model\n","      loaded_model = load_saved_transformer(filepath = experiment_dir)\n","\n","      # Infer using loaded model\n","      # The train and validation datasets are dummy in this case, because \n","      # preprocess was called with train_mode = False above.\n","      evaluation_scores, y_pred_proba, y_pred, y_test, clf_rep = infer(loaded_model,\n","                                                                    preprocess.tf_test_dataset,\n","                                                                    preprocess.tokenized_dataset,\n","                                                                    preprocess.batch_size,\n","                                                                    threshold = 0.5, \n","                                                                    anomalies = preprocess.anomalies)\n","      \n","      # Convert the classification report in form of a pd.DataFrame, adding \n","      # additional info\n","      clf_rep_df = convert_clf_rep_to_df_multilabel_BERT_kw_args(clf_rep, \n","                                                              preprocess.anomalies, \n","                                                              preprocessing = 'original',\n","                                                              classifier = 'BERT_BASE',\n","                                                              undersampling = 0,\n","                                                              UNfrozen_layers = '9,10,11,12',\n","                                                              concat_layers = 'None',\n","                                                              comments = 'inference_on_FINAL_test_set',\n","                                                              experiment_ID = '11_3_3'\n","                                                              )\n","      \n","      ###############################\n","      # SAVE THE MULTILABEL OUTPUTS\n","      save_ML_outputs(dir_name, experiment_name + '_infer_FINAL_test_set', \n","                  y_pred_proba = y_pred_proba, \n","                  y_pred = y_pred, \n","                  y_test = y_test, \n","                  clf_rep = clf_rep,\n","                  clf_rep_df = clf_rep_df)"],"metadata":{"id":"1Akd5H76MrpN","executionInfo":{"status":"ok","timestamp":1666526189759,"user_tz":-120,"elapsed":14,"user":{"displayName":"Project Datascientest","userId":"15618889726029501374"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["train_load_transformer_model(dir_name = '/content/drive/MyDrive/data/saved models/Yannis/BERT/2022_10_23_11_3_3_BERT_classes_reproduce_7_3_9_3/',\n","          experiment_name = '11_3_3',\n","          df = df, \n","          train_mode = True, \n","          num_epochs = 20,\n","          load_model = False, \n","          save_and_overwrite_model = True)"],"metadata":{"id":"YjRjuIAYWOO0"},"execution_count":null,"outputs":[]}]}